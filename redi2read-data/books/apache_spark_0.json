[{"pageCount":606,"thumbnail":"http:\/\/books.google.com\/books\/content?id=pitLDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":40.79,"subtitle":"Big Data Processing Made Simple","description":"Learn how to use, deploy, and maintain Apache Spark with this comprehensive guide, written by the creators of the open-source cluster-computing framework. With an emphasis on improvements and new features in Spark 2.0, authors Bill Chambers and Matei Zaharia break down Spark topics into distinct sections, each with unique goals. You\u2019ll explore the basic operations and common functions of Spark\u2019s structured APIs, as well as Structured Streaming, a new high-level API for building end-to-end streaming applications. Developers and system administrators will learn the fundamentals of monitoring, tuning, and debugging Spark, and explore machine learning techniques and scenarios for employing MLlib, Spark\u2019s scalable machine-learning library. Get a gentle overview of big data and Spark Learn about DataFrames, SQL, and Datasets\u2014Spark\u2019s core APIs\u2014through worked examples Dive into Spark\u2019s low-level APIs, RDDs, and execution of SQL and DataFrames Understand how Spark runs on a cluster Debug, monitor, and tune Spark clusters and applications Learn the power of Structured Streaming, Spark\u2019s stream-processing engine Learn how you can apply MLlib to a variety of problems, including classification or recommendation","language":"en","currency":"USD","id":"1491912294","title":"Spark: The Definitive Guide","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=pitLDwAAQBAJ&source=gbs_api","authors":["Bill Chambers","Matei Zaharia"]},{"pageCount":393,"thumbnail":"http:\/\/books.google.com\/books\/content?id=wzppDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.72,"subtitle":"With Resilient Distributed Datasets, Spark SQL, Structured Streaming and Spark Machine Learning library","description":"Develop applications for the big data landscape with Spark and Hadoop. This book also explains the role of Spark in developing scalable machine learning and analytics applications with Cloud technologies. Beginning Apache Spark 2 gives you an introduction to Apache Spark and shows you how to work with it. Along the way, you\u2019ll discover resilient distributed datasets (RDDs); use Spark SQL for structured data; and learn stream processing and build real-time applications with Spark Structured Streaming. Furthermore, you\u2019ll learn the fundamentals of Spark ML for machine learning and much more. After you read this book, you will have the fundamentals to become proficient in using Apache Spark and know when and how to apply it to your big data applications. What You Will Learn Understand Spark unified data processing platform How to run Spark in Spark Shell or Databricks Use and manipulate RDDs Deal with structured data using Spark SQL through its operations and advanced functions Build real-time applications using Spark Structured Streaming Develop intelligent applications with the Spark Machine Learning library Who This Book Is For Programmers and developers active in big data, Hadoop, and Java but who are new to the Apache Spark platform.","language":"en","currency":"USD","id":"1484235797","title":"Beginning Apache Spark 2","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=wzppDwAAQBAJ&source=gbs_api","authors":["Hien Luu"]},{"pageCount":452,"thumbnail":"http:\/\/books.google.com\/books\/content?id=GrKbDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":47.39,"subtitle":"Mastering Structured Streaming and Spark Streaming","description":"Before you can build analytics tools to gain quick insights, you first need to know how to process data in real time. With this practical guide, developers familiar with Apache Spark will learn how to put this in-memory framework to use for streaming data. You\u2019ll discover how Spark enables you to write streaming jobs in almost the same way you write batch jobs. Authors Gerard Maas and Fran√ßois Garillot help you explore the theoretical underpinnings of Apache Spark. This comprehensive guide features two sections that compare and contrast the streaming APIs Spark now supports: the original Spark Streaming library and the newer Structured Streaming API. Learn fundamental stream processing concepts and examine different streaming architectures Explore Structured Streaming through practical examples; learn different aspects of stream processing in detail Create and operate streaming jobs and applications with Spark Streaming; integrate Spark Streaming with other Spark APIs Learn advanced Spark Streaming techniques, including approximation algorithms and machine learning algorithms Compare Apache Spark to other stream processing projects, including Apache Storm, Apache Flink, and Apache Kafka Streams","language":"en","currency":"USD","id":"1491944196","title":"Stream Processing with Apache Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=GrKbDwAAQBAJ&source=gbs_api","authors":["Gerard Maas","Francois Garillot"]},{"pageCount":318,"thumbnail":"http:\/\/books.google.com\/books\/content?id=ENZOCwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.72,"subtitle":null,"description":"Gain expertise in processing and storing data by using advanced techniques with Apache Spark About This Book Explore the integration of Apache Spark with third party applications such as H20, Databricks and Titan Evaluate how Cassandra and Hbase can be used for storage An advanced guide with a combination of instructions and practical examples to extend the most up-to date Spark functionalities Who This Book Is For If you are a developer with some experience with Spark and want to strengthen your knowledge of how to get around in the world of Spark, then this book is ideal for you. Basic knowledge of Linux, Hadoop and Spark is assumed. Reasonable knowledge of Scala is expected. What You Will Learn Extend the tools available for processing and storage Examine clustering and classification using MLlib Discover Spark stream processing via Flume, HDFS Create a schema in Spark SQL, and learn how a Spark schema can be populated with data Study Spark based graph processing using Spark GraphX Combine Spark with H20 and deep learning and learn why it is useful Evaluate how graph storage works with Apache Spark, Titan, HBase and Cassandra Use Apache Spark in the cloud with Databricks and AWS In Detail Apache Spark is an in-memory cluster based parallel processing system that provides a wide range of functionality like graph processing, machine learning, stream processing and SQL. It operates at unprecedented speeds, is easy to use and offers a rich set of data transformations. This book aims to take your limited knowledge of Spark to the next level by teaching you how to expand Spark functionality. The book commences with an overview of the Spark eco-system. You will learn how to use MLlib to create a fully working neural net for handwriting recognition. You will then discover how stream processing can be tuned for optimal performance and to ensure parallel processing. The book extends to show how to incorporate H20 for machine learning, Titan for graph based storage, Databricks for cloud-based Spark. Intermediate Scala based code examples are provided for Apache Spark module processing in a CentOS Linux and Databricks cloud environment. Style and approach This book is an extensive guide to Apache Spark modules and tools and shows how Spark's functionality can be extended for real-time processing and storage with worked examples.","language":"en","currency":"USD","id":"1783987154","title":"Mastering Apache Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=ENZOCwAAQBAJ&source=gbs_api","authors":["Mike Frampton"]},{"pageCount":350,"thumbnail":"http:\/\/books.google.com\/books\/content?id=8-ZDDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":null,"description":"Unleash the data processing and analytics capability of Apache Spark with the language of choice: Java About This Book Perform big data processing with Spark\u2014without having to learn Scala! Use the Spark Java API to implement efficient enterprise-grade applications for data processing and analytics Go beyond mainstream data processing by adding querying capability, Machine Learning, and graph processing using Spark Who This Book Is For If you are a Java developer interested in learning to use the popular Apache Spark framework, this book is the resource you need to get started. Apache Spark developers who are looking to build enterprise-grade applications in Java will also find this book very useful. What You Will Learn Process data using different file formats such as XML, JSON, CSV, and plain and delimited text, using the Spark core Library. Perform analytics on data from various data sources such as Kafka, and Flume using Spark Streaming Library Learn SQL schema creation and the analysis of structured data using various SQL functions including Windowing functions in the Spark SQL Library Explore Spark Mlib APIs while implementing Machine Learning techniques to solve real-world problems Get to know Spark GraphX so you understand various graph-based analytics that can be performed with Spark In Detail Apache Spark is the buzzword in the big data industry right now, especially with the increasing need for real-time streaming and data processing. While Spark is built on Scala, the Spark Java API exposes all the Spark features available in the Scala version for Java developers. This book will show you how you can implement various functionalities of the Apache Spark framework in Java, without stepping out of your comfort zone. The book starts with an introduction to the Apache Spark 2.x ecosystem, followed by explaining how to install and configure Spark, and refreshes the Java concepts that will be useful to you when consuming Apache Spark's APIs. You will explore RDD and its associated common Action and Transformation Java APIs, set up a production-like clustered environment, and work with Spark SQL. Moving on, you will perform near-real-time processing with Spark streaming, Machine Learning analytics with Spark MLlib, and graph processing with GraphX, all using various Java packages. By the end of the book, you will have a solid foundation in implementing components in the Spark framework in Java to build fast, real-time applications. Style and approach This practical guide teaches readers the fundamentals of the Apache Spark framework and how to implement components using the Java language. It is a unique blend of theory and practical examples, and is written in a way that will gradually build your knowledge of Apache Spark.","language":"en","currency":"USD","id":"178712942X","title":"Apache Spark 2.x for Java Developers","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=8-ZDDwAAQBAJ&source=gbs_api","authors":["Sourav Gulati","Sumit Kumar"]},{"pageCount":280,"thumbnail":"http:\/\/books.google.com\/books\/content?id=wst-DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":39.49,"subtitle":"Using the Scala API","description":"Work with Apache Spark using Scala to deploy and set up single-node, multi-node, and high-availability clusters. This book discusses various components of Spark such as Spark Core, DataFrames, Datasets and SQL, Spark Streaming, Spark MLib, and R on Spark with the help of practical code snippets for each topic. Practical Apache Spark also covers the integration of Apache Spark with Kafka with examples. You\u2019ll follow a learn-to-do-by-yourself approach to learning \u2013 learn the concepts, practice the code snippets in Scala, and complete the assignments given to get an overall exposure. On completion, you\u2019ll have knowledge of the functional programming aspects of Scala, and hands-on expertise in various Spark components. You\u2019ll also become familiar with machine learning algorithms with real-time usage. What You Will Learn Discover the functional programming features of Scala Understand the complete architecture of Spark and its components Integrate Apache Spark with Hive and Kafka Use Spark SQL, DataFrames, and Datasets to process data using traditional SQL queries Work with different machine learning concepts and libraries using Spark's MLlib packages Who This Book Is For Developers and professionals who deal with batch and stream data processing.","language":"en","currency":"USD","id":"1484236521","title":"Practical Apache Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=wst-DwAAQBAJ&source=gbs_api","authors":["Subhashini Chellappan","Dharanitharan Ganesan"]},{"pageCount":445,"thumbnail":"http:\/\/books.google.com\/books\/content?id=sNPvDAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":null,"description":"Apache Spark is a fast, scalable, and flexible open source distributed processing engine for big data systems and is one of the most active open source big data projects to date. In just 24 lessons of one hour or less, Sams Teach Yourself Apache Spark in 24 Hours helps you build practical Big Data solutions that leverage Spark\u2019s amazing speed, scalability, simplicity, and versatility. This book\u2019s straightforward, step-by-step approach shows you how to deploy, program, optimize, manage, integrate, and extend Spark\u2013now, and for years to come. You\u2019ll discover how to create powerful solutions encompassing cloud computing, real-time stream processing, machine learning, and more. Every lesson builds on what you\u2019ve already learned, giving you a rock-solid foundation for real-world success. Whether you are a data analyst, data engineer, data scientist, or data steward, learning Spark will help you to advance your career or embark on a new career in the booming area of Big Data. Learn how to \u2022 Discover what Apache Spark does and how it fits into the Big Data landscape \u2022 Deploy and run Spark locally or in the cloud \u2022 Interact with Spark from the shell \u2022 Make the most of the Spark Cluster Architecture \u2022 Develop Spark applications with Scala and functional Python \u2022 Program with the Spark API, including transformations and actions \u2022 Apply practical data engineering\/analysis approaches designed for Spark \u2022 Use Resilient Distributed Datasets (RDDs) for caching, persistence, and output \u2022 Optimize Spark solution performance \u2022 Use Spark with SQL (via Spark SQL) and with NoSQL (via Cassandra) \u2022 Leverage cutting-edge functional programming techniques \u2022 Extend Spark with streaming, R, and Sparkling Water \u2022 Start building Spark-based machine learning and graph-processing applications \u2022 Explore advanced messaging technologies, including Kafka \u2022 Preview and prepare for Spark\u2019s next generation of innovations Instructions walk you through common questions, issues, and tasks; Q-and-As, Quizzes, and Exercises build and test your knowledge; \"Did You Know?\" tips offer insider advice and shortcuts; and \"Watch Out!\" alerts help you avoid pitfalls. By the time you're finished, you'll be comfortable using Apache Spark to solve a wide spectrum of Big Data problems.","language":"en","currency":"USD","id":"0134445821","title":"Apache Spark in 24 Hours, Sams Teach Yourself","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=sNPvDAAAQBAJ&source=gbs_api","authors":["Jeffrey Aven"]},{"pageCount":356,"thumbnail":"http:\/\/books.google.com\/books\/content?id=iLkrDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":22.39,"subtitle":null,"description":"Learn about the fastest-growing open source project in the world, and find out how it revolutionizes big data analytics About This Book Exclusive guide that covers how to get up and running with fast data processing using Apache Spark Explore and exploit various possibilities with Apache Spark using real-world use cases in this book Want to perform efficient data processing at real time? This book will be your one-stop solution. Who This Book Is For This guide appeals to big data engineers, analysts, architects, software engineers, even technical managers who need to perform efficient data processing on Hadoop at real time. Basic familiarity with Java or Scala will be helpful. The assumption is that readers will be from a mixed background, but would be typically people with background in engineering\/data science with no prior Spark experience and want to understand how Spark can help them on their analytics journey. What You Will Learn Get an overview of big data analytics and its importance for organizations and data professionals Delve into Spark to see how it is different from existing processing platforms Understand the intricacies of various file formats, and how to process them with Apache Spark. Realize how to deploy Spark with YARN, MESOS or a Stand-alone cluster manager. Learn the concepts of Spark SQL, SchemaRDD, Caching and working with Hive and Parquet file formats Understand the architecture of Spark MLLib while discussing some of the off-the-shelf algorithms that come with Spark. Introduce yourself to the deployment and usage of SparkR. Walk through the importance of Graph computation and the graph processing systems available in the market Check the real world example of Spark by building a recommendation engine with Spark using ALS. Use a Telco data set, to predict customer churn using Random Forests. In Detail Spark juggernaut keeps on rolling and getting more and more momentum each day. Spark provides key capabilities in the form of Spark SQL, Spark Streaming, Spark ML and Graph X all accessible via Java, Scala, Python and R. Deploying the key capabilities is crucial whether it is on a Standalone framework or as a part of existing Hadoop installation and configuring with Yarn and Mesos. The next part of the journey after installation is using key components, APIs, Clustering, machine learning APIs, data pipelines, parallel programming. It is important to understand why each framework component is key, how widely it is being used, its stability and pertinent use cases. Once we understand the individual components, we will take a couple of real life advanced analytics examples such as 'Building a Recommendation system', 'Predicting customer churn' and so on. The objective of these real life examples is to give the reader confidence of using Spark for real-world problems. Style and approach With the help of practical examples and real-world use cases, this guide will take you from scratch to building efficient data applications using Apache Spark. You will learn all about this excellent data processing engine in a step-by-step manner, taking one aspect of it at a time. This highly practical guide will include how to work with data pipelines, dataframes, clustering, SparkSQL, parallel programming, and such insightful topics with the help of real-world use cases.","language":"en","currency":"USD","id":"1785889583","title":"Learning Apache Spark 2","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=iLkrDwAAQBAJ&source=gbs_api","authors":["Muhammad Asif Abbasi"]},{"pageCount":268,"thumbnail":"http:\/\/books.google.com\/books\/content?id=z4WZDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":47.39,"subtitle":"Practical Examples in Apache Spark and Neo4j","description":"Discover how graph algorithms can help you leverage the relationships within your data to develop more intelligent solutions and enhance your machine learning models. You\u2019ll learn how graph analytics are uniquely suited to unfold complex structures and reveal difficult-to-find patterns lurking in your data. Whether you are trying to build dynamic network models or forecast real-world behavior, this book illustrates how graph algorithms deliver value\u2014from finding vulnerabilities and bottlenecks to detecting communities and improving machine learning predictions. This practical book walks you through hands-on examples of how to use graph algorithms in Apache Spark and Neo4j\u2014two of the most common choices for graph analytics. Also included: sample code and tips for over 20 practical graph algorithms that cover optimal pathfinding, importance through centrality, and community detection. Learn how graph analytics vary from conventional statistical analysis Understand how classic graph algorithms work, and how they are applied Get guidance on which algorithms to use for different types of questions Explore algorithm examples with working code and sample datasets from Spark and Neo4j See how connected feature extraction can increase machine learning accuracy and precision Walk through creating an ML workflow for link prediction combining Neo4j and Spark","language":"en","currency":"USD","id":"1492047635","title":"Graph Algorithms","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=z4WZDwAAQBAJ&source=gbs_api","authors":["Mark Needham","Amy E. Hodler"]},{"pageCount":358,"thumbnail":"http:\/\/books.google.com\/books\/content?id=o0IlDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":"Best Practices for Scaling and Optimizing Apache Spark","description":"Apache Spark is amazing when everything clicks. But if you haven\u2019t seen the performance improvements you expected, or still don\u2019t feel confident enough to use Spark in production, this practical book is for you. Authors Holden Karau and Rachel Warren demonstrate performance optimizations to help your Spark queries run faster and handle larger data sizes, while using fewer resources. Ideal for software engineers, data engineers, developers, and system administrators working with large-scale data applications, this book describes techniques that can reduce data infrastructure costs and developer hours. Not only will you gain a more comprehensive understanding of Spark, you\u2019ll also learn how to make it sing. With this book, you\u2019ll explore: How Spark SQL\u2019s new interfaces improve performance over SQL\u2019s RDD data structure The choice between data joins in Core Spark and Spark SQL Techniques for getting the most out of standard RDD transformations How to work around performance issues in Spark\u2019s key\/value pair paradigm Writing high-performance Spark code without Scala or the JVM How to test for functionality and performance when applying suggested improvements Using Spark MLlib and Spark ML machine learning libraries Spark\u2019s Streaming components and external community packages","language":"en","currency":"USD","id":"1491943157","title":"High Performance Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=o0IlDwAAQBAJ&source=gbs_api","authors":["Holden Karau","Rachel Warren"]},{"pageCount":616,"thumbnail":"http:\/\/books.google.com\/books\/content?id=ANGBDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":"Master complex big data processing, stream analytics, and machine learning with Apache Spark","description":"Build efficient data flow and machine learning programs with this flexible, multi-functional open-source cluster-computing framework Key Features Master the art of real-time big data processing and machine learning Explore a wide range of use-cases to analyze large data Discover ways to optimize your work by using many features of Spark 2.x and Scala Book Description Apache Spark is an in-memory, cluster-based data processing system that provides a wide range of functionalities such as big data processing, analytics, machine learning, and more. With this Learning Path, you can take your knowledge of Apache Spark to the next level by learning how to expand Spark's functionality and building your own data flow and machine learning programs on this platform. You will work with the different modules in Apache Spark, such as interactive querying with Spark SQL, using DataFrames and datasets, implementing streaming analytics with Spark Streaming, and applying machine learning and deep learning techniques on Spark using MLlib and various external tools. By the end of this elaborately designed Learning Path, you will have all the knowledge you need to master Apache Spark, and build your own big data processing and analytics pipeline quickly and without any hassle. This Learning Path includes content from the following Packt products: Mastering Apache Spark 2.x by Romeo Kienzler Scala and Spark for Big Data Analytics by Md. Rezaul Karim, Sridhar Alla Apache Spark 2.x Machine Learning Cookbook by Siamak Amirghodsi, Meenakshi Rajendran, Broderick Hall, Shuen MeiCookbook What you will learn Get to grips with all the features of Apache Spark 2.x Perform highly optimized real-time big data processing Use ML and DL techniques with Spark MLlib and third-party tools Analyze structured and unstructured data using SparkSQL and GraphX Understand tuning, debugging, and monitoring of big data applications Build scalable and fault-tolerant streaming applications Develop scalable recommendation engines Who this book is for If you are an intermediate-level Spark developer looking to master the advanced capabilities and use-cases of Apache Spark 2.x, this Learning Path is ideal for you. Big data professionals who want to learn how to integrate and use the features of Apache Spark and build a strong big data pipeline will also find this Learning Path useful. To grasp the concepts explained in this Learning Path, you must know the fundamentals of Apache Spark and Scala.","language":"en","currency":"USD","id":"1789959918","title":"Apache Spark 2: Data Processing and Real-Time Analytics","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=ANGBDwAAQBAJ&source=gbs_api","authors":["Romeo Kienzler","Md. Rezaul Karim","Sridhar Alla","Siamak Amirghodsi","Meenakshi Rajendran","Broderick Hall","Shuen Mei"]},{"pageCount":296,"thumbnail":"http:\/\/books.google.com\/books\/content?id=8Hc5DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":null,"description":"Frank Kane's hands-on Spark training course, based on his bestselling Taming Big Data with Apache Spark and Python video, now available in a book. Understand and analyze large data sets using Spark on a single system or on a cluster. About This Book Understand how Spark can be distributed across computing clusters Develop and run Spark jobs efficiently using Python A hands-on tutorial by Frank Kane with over 15 real-world examples teaching you Big Data processing with Spark Who This Book Is For If you are a data scientist or data analyst who wants to learn Big Data processing using Apache Spark and Python, this book is for you. If you have some programming experience in Python, and want to learn how to process large amounts of data using Apache Spark, Frank Kane's Taming Big Data with Apache Spark and Python will also help you. What You Will Learn Find out how you can identify Big Data problems as Spark problems Install and run Apache Spark on your computer or on a cluster Analyze large data sets across many CPUs using Spark's Resilient Distributed Datasets Implement machine learning on Spark using the MLlib library Process continuous streams of data in real time using the Spark streaming module Perform complex network analysis using Spark's GraphX library Use Amazon's Elastic MapReduce service to run your Spark jobs on a cluster In Detail Frank Kane's Taming Big Data with Apache Spark and Python is your companion to learning Apache Spark in a hands-on manner. Frank will start you off by teaching you how to set up Spark on a single system or on a cluster, and you'll soon move on to analyzing large data sets using Spark RDD, and developing and running effective Spark jobs quickly using Python. Apache Spark has emerged as the next big thing in the Big Data domain \u2013 quickly rising from an ascending technology to an established superstar in just a matter of years. Spark allows you to quickly extract actionable insights from large amounts of data, on a real-time basis, making it an essential tool in many modern businesses. Frank has packed this book with over 15 interactive, fun-filled examples relevant to the real world, and he will empower you to understand the Spark ecosystem and implement production-grade real-time Spark projects with ease. Style and approach Frank Kane's Taming Big Data with Apache Spark and Python is a hands-on tutorial with over 15 real-world examples carefully explained by Frank in a step-by-step manner. The examples vary in complexity, and you can move through them at your own pace.","language":"en","currency":"USD","id":"1787288307","title":"Frank Kane's Taming Big Data with Apache Spark and Python","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=8Hc5DwAAQBAJ&source=gbs_api","authors":["Frank Kane"]},{"pageCount":322,"thumbnail":"http:\/\/books.google.com\/books\/content?id=iiWGDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":22.39,"subtitle":"Build and deploy distributed deep learning applications on Apache Spark","description":"Speed up the design and implementation of deep learning solutions using Apache Spark Key Features Explore the world of distributed deep learning with Apache Spark Train neural networks with deep learning libraries such as BigDL and TensorFlow Develop Spark deep learning applications to intelligently handle large and complex datasets Book Description Deep learning is a subset of machine learning where datasets with several layers of complexity can be processed. Hands-On Deep Learning with Apache Spark addresses the sheer complexity of technical and analytical parts and the speed at which deep learning solutions can be implemented on Apache Spark. The book starts with the fundamentals of Apache Spark and deep learning. You will set up Spark for deep learning, learn principles of distributed modeling, and understand different types of neural nets. You will then implement deep learning models, such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) on Spark. As you progress through the book, you will gain hands-on experience of what it takes to understand the complex datasets you are dealing with. During the course of this book, you will use popular deep learning frameworks, such as TensorFlow, Deeplearning4j, and Keras to train your distributed models. By the end of this book, you'll have gained experience with the implementation of your models on a variety of use cases. What you will learn Understand the basics of deep learning Set up Apache Spark for deep learning Understand the principles of distribution modeling and different types of neural networks Obtain an understanding of deep learning algorithms Discover textual analysis and deep learning with Spark Use popular deep learning frameworks, such as Deeplearning4j, TensorFlow, and Keras Explore popular deep learning algorithms Who this book is for If you are a Scala developer, data scientist, or data analyst who wants to learn how to use Spark for implementing efficient deep learning models, Hands-On Deep Learning with Apache Spark is for you. Knowledge of the core machine learning concepts and some exposure to Spark will be helpful.","language":"en","currency":"USD","id":"1788999703","title":"Hands-On Deep Learning with Apache Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=iiWGDwAAQBAJ&source=gbs_api","authors":["Guglielmo Iozzia"]},{"pageCount":142,"thumbnail":"http:\/\/books.google.com\/books\/content?id=Uuh1DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":14.59,"subtitle":"Efficiently tackle large datasets and big data analysis with Spark and Python","description":"No need to spend hours ploughing through endless data \u2013 let Spark, one of the fastest big data processing engines available, do the hard work for you. Key Features Get up and running with Apache Spark and Python Integrate Spark with AWS for real-time analytics Apply processed data streams to machine learning APIs of Apache Spark Book Description Processing big data in real time is challenging due to scalability, information consistency, and fault-tolerance. This book teaches you how to use Spark to make your overall analytical workflow faster and more efficient. You'll explore all core concepts and tools within the Spark ecosystem, such as Spark Streaming, the Spark Streaming API, machine learning extension, and structured streaming. You'll begin by learning data processing fundamentals using Resilient Distributed Datasets (RDDs), SQL, Datasets, and Dataframes APIs. After grasping these fundamentals, you'll move on to using Spark Streaming APIs to consume data in real time from TCP sockets, and integrate Amazon Web Services (AWS) for stream consumption. By the end of this book, you\u2019ll not only have understood how to use machine learning extensions and structured streams but you\u2019ll also be able to apply Spark in your own upcoming big data projects. What you will learn Write your own Python programs that can interact with Spark Implement data stream consumption using Apache Spark Recognize common operations in Spark to process known data streams Integrate Spark streaming with Amazon Web Services (AWS) Create a collaborative filtering model with the movielens dataset Apply processed data streams to Spark machine learning APIs Who this book is for Data Processing with Apache Spark is for you if you are a software engineer, architect, or IT professional who wants to explore distributed systems and big data analytics. Although you don\u2018t need any knowledge of Spark, prior experience of working with Python is recommended.","language":"en","currency":"USD","id":"1789804523","title":"Big Data Processing with Apache Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=Uuh1DwAAQBAJ&source=gbs_api","authors":["Manuel Ignacio Franco Galeano"]},{"pageCount":240,"thumbnail":"http:\/\/books.google.com\/books\/content?id=0Z2BDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":14.59,"subtitle":"Uncover patterns, derive actionable insights, and learn from big data using MLlib","description":"Combine advanced analytics including Machine Learning, Deep Learning Neural Networks and Natural Language Processing with modern scalable technologies including Apache Spark to derive actionable insights from Big Data in real-time Key Features Make a hands-on start in the fields of Big Data, Distributed Technologies and Machine Learning Learn how to design, develop and interpret the results of common Machine Learning algorithms Uncover hidden patterns in your data in order to derive real actionable insights and business value Book Description Every person and every organization in the world manages data, whether they realize it or not. Data is used to describe the world around us and can be used for almost any purpose, from analyzing consumer habits to fighting disease and serious organized crime. Ultimately, we manage data in order to derive value from it, and many organizations around the world have traditionally invested in technology to help process their data faster and more efficiently. But we now live in an interconnected world driven by mass data creation and consumption where data is no longer rows and columns restricted to a spreadsheet, but an organic and evolving asset in its own right. With this realization comes major challenges for organizations: how do we manage the sheer size of data being created every second (think not only spreadsheets and databases, but also social media posts, images, videos, music, blogs and so on)? And once we can manage all of this data, how do we derive real value from it? The focus of Machine Learning with Apache Spark is to help us answer these questions in a hands-on manner. We introduce the latest scalable technologies to help us manage and process big data. We then introduce advanced analytical algorithms applied to real-world use cases in order to uncover patterns, derive actionable insights, and learn from this big data. What you will learn Understand how Spark fits in the context of the big data ecosystem Understand how to deploy and configure a local development environment using Apache Spark Understand how to design supervised and unsupervised learning models Build models to perform NLP, deep learning, and cognitive services using Spark ML libraries Design real-time machine learning pipelines in Apache Spark Become familiar with advanced techniques for processing a large volume of data by applying machine learning algorithms Who this book is for This book is aimed at Business Analysts, Data Analysts and Data Scientists who wish to make a hands-on start in order to take advantage of modern Big Data technologies combined with Advanced Analytics.","language":"en","currency":"USD","id":"1789349370","title":"Machine Learning with Apache Spark Quick Start Guide","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=0Z2BDwAAQBAJ&source=gbs_api","authors":["Jillur Quddus"]},{"pageCount":474,"thumbnail":"http:\/\/books.google.com\/books\/content?id=NA5lDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":"Over 80 recipes that streamline deep learning in a distributed environment with Apache Spark","description":"A solution-based guide to put your deep learning models into production with the power of Apache Spark Key Features Discover practical recipes for distributed deep learning with Apache Spark Learn to use libraries such as Keras and TensorFlow Solve problems in order to train your deep learning models on Apache Spark Book Description With deep learning gaining rapid mainstream adoption in modern-day industries, organizations are looking for ways to unite popular big data tools with highly efficient deep learning libraries. As a result, this will help deep learning models train with higher efficiency and speed. With the help of the Apache Spark Deep Learning Cookbook, you\u2019ll work through specific recipes to generate outcomes for deep learning algorithms, without getting bogged down in theory. From setting up Apache Spark for deep learning to implementing types of neural net, this book tackles both common and not so common problems to perform deep learning on a distributed environment. In addition to this, you\u2019ll get access to deep learning code within Spark that can be reused to answer similar problems or tweaked to answer slightly different problems. You will also learn how to stream and cluster your data with Spark. Once you have got to grips with the basics, you\u2019ll explore how to implement and deploy deep learning models, such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) in Spark, using popular libraries such as TensorFlow and Keras. By the end of the book, you'll have the expertise to train and deploy efficient deep learning models on Apache Spark. What you will learn Set up a fully functional Spark environment Understand practical machine learning and deep learning concepts Apply built-in machine learning libraries within Spark Explore libraries that are compatible with TensorFlow and Keras Explore NLP models such as Word2vec and TF-IDF on Spark Organize dataframes for deep learning evaluation Apply testing and training modeling to ensure accuracy Access readily available code that may be reusable Who this book is for If you\u2019re looking for a practical and highly useful resource for implementing efficiently distributed deep learning models with Apache Spark, then the Apache Spark Deep Learning Cookbook is for you. Knowledge of the core machine learning concepts and a basic understanding of the Apache Spark framework is required to get the best out of this book. Additionally, some programming knowledge in Python is a plus.","language":"en","currency":"USD","id":"1788471555","title":"Apache Spark Deep Learning Cookbook","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=NA5lDwAAQBAJ&source=gbs_api","authors":["Ahmed Sherif","Amrith Ravindra"]},{"pageCount":154,"thumbnail":"http:\/\/books.google.com\/books\/content?id=mmSGDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":14.59,"subtitle":"Quickly learn the art of writing efficient big data applications with Apache Spark","description":"A practical guide for solving complex data processing challenges by applying the best optimizations techniques in Apache Spark. Key Features Learn about the core concepts and the latest developments in Apache Spark Master writing efficient big data applications with Spark\u2019s built-in modules for SQL, Streaming, Machine Learning and Graph analysis Get introduced to a variety of optimizations based on the actual experience Book Description Apache Spark is a flexible framework that allows processing of batch and real-time data. Its unified engine has made it quite popular for big data use cases. This book will help you to get started with Apache Spark 2.0 and write big data applications for a variety of use cases. It will also introduce you to Apache Spark \u2013 one of the most popular Big Data processing frameworks. Although this book is intended to help you get started with Apache Spark, but it also focuses on explaining the core concepts. This practical guide provides a quick start to the Spark 2.0 architecture and its components. It teaches you how to set up Spark on your local machine. As we move ahead, you will be introduced to resilient distributed datasets (RDDs) and DataFrame APIs, and their corresponding transformations and actions. Then, we move on to the life cycle of a Spark application and learn about the techniques used to debug slow-running applications. You will also go through Spark\u2019s built-in modules for SQL, streaming, machine learning, and graph analysis. Finally, the book will lay out the best practices and optimization techniques that are key for writing efficient Spark applications. By the end of this book, you will have a sound fundamental understanding of the Apache Spark framework and you will be able to write and optimize Spark applications. What you will learn Learn core concepts such as RDDs, DataFrames, transformations, and more Set up a Spark development environment Choose the right APIs for your applications Understand Spark\u2019s architecture and the execution flow of a Spark application Explore built-in modules for SQL, streaming, ML, and graph analysis Optimize your Spark job for better performance Who this book is for If you are a big data enthusiast and love processing huge amount of data, this book is for you. If you are data engineer and looking for the best optimization techniques for your Spark applications, then you will find this book helpful. This book also helps data scientists who want to implement their machine learning algorithms in Spark. You need to have a basic understanding of any one of the programming languages such as Scala, Python or Java.","language":"en","currency":"USD","id":"178934266X","title":"Apache Spark Quick Start Guide","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=mmSGDwAAQBAJ&source=gbs_api","authors":["Shrey Mehrotra","Akash Grade"]},{"pageCount":318,"thumbnail":"http:\/\/books.google.com\/books\/content?id=ENZOCwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.72,"subtitle":null,"description":"Gain expertise in processing and storing data by using advanced techniques with Apache Spark About This Book Explore the integration of Apache Spark with third party applications such as H20, Databricks and Titan Evaluate how Cassandra and Hbase can be used for storage An advanced guide with a combination of instructions and practical examples to extend the most up-to date Spark functionalities Who This Book Is For If you are a developer with some experience with Spark and want to strengthen your knowledge of how to get around in the world of Spark, then this book is ideal for you. Basic knowledge of Linux, Hadoop and Spark is assumed. Reasonable knowledge of Scala is expected. What You Will Learn Extend the tools available for processing and storage Examine clustering and classification using MLlib Discover Spark stream processing via Flume, HDFS Create a schema in Spark SQL, and learn how a Spark schema can be populated with data Study Spark based graph processing using Spark GraphX Combine Spark with H20 and deep learning and learn why it is useful Evaluate how graph storage works with Apache Spark, Titan, HBase and Cassandra Use Apache Spark in the cloud with Databricks and AWS In Detail Apache Spark is an in-memory cluster based parallel processing system that provides a wide range of functionality like graph processing, machine learning, stream processing and SQL. It operates at unprecedented speeds, is easy to use and offers a rich set of data transformations. This book aims to take your limited knowledge of Spark to the next level by teaching you how to expand Spark functionality. The book commences with an overview of the Spark eco-system. You will learn how to use MLlib to create a fully working neural net for handwriting recognition. You will then discover how stream processing can be tuned for optimal performance and to ensure parallel processing. The book extends to show how to incorporate H20 for machine learning, Titan for graph based storage, Databricks for cloud-based Spark. Intermediate Scala based code examples are provided for Apache Spark module processing in a CentOS Linux and Databricks cloud environment. Style and approach This book is an extensive guide to Apache Spark modules and tools and shows how Spark's functionality can be extended for real-time processing and storage with worked examples.","language":"en","currency":"USD","id":"1783987154","title":"Mastering Apache Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=ENZOCwAAQBAJ&source=gbs_api","authors":["Mike Frampton"]},{"pageCount":280,"thumbnail":"http:\/\/books.google.com\/books\/content?id=wst-DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":39.49,"subtitle":"Using the Scala API","description":"Work with Apache Spark using Scala to deploy and set up single-node, multi-node, and high-availability clusters. This book discusses various components of Spark such as Spark Core, DataFrames, Datasets and SQL, Spark Streaming, Spark MLib, and R on Spark with the help of practical code snippets for each topic. Practical Apache Spark also covers the integration of Apache Spark with Kafka with examples. You\u2019ll follow a learn-to-do-by-yourself approach to learning \u2013 learn the concepts, practice the code snippets in Scala, and complete the assignments given to get an overall exposure. On completion, you\u2019ll have knowledge of the functional programming aspects of Scala, and hands-on expertise in various Spark components. You\u2019ll also become familiar with machine learning algorithms with real-time usage. What You Will Learn Discover the functional programming features of Scala Understand the complete architecture of Spark and its components Integrate Apache Spark with Hive and Kafka Use Spark SQL, DataFrames, and Datasets to process data using traditional SQL queries Work with different machine learning concepts and libraries using Spark's MLlib packages Who This Book Is For Developers and professionals who deal with batch and stream data processing.","language":"en","currency":"USD","id":"1484236521","title":"Practical Apache Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=wst-DwAAQBAJ&source=gbs_api","authors":["Subhashini Chellappan","Dharanitharan Ganesan"]},{"pageCount":332,"thumbnail":"http:\/\/books.google.com\/books\/content?id=Q5XcDgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":null,"description":"Develop large-scale distributed data processing applications using Spark 2 in Scala and Python About This Book This book offers an easy introduction to the Spark framework published on the latest version of Apache Spark 2 Perform efficient data processing, machine learning and graph processing using various Spark components A practical guide aimed at beginners to get them up and running with Spark Who This Book Is For If you are an application developer, data scientist, or big data solutions architect who is interested in combining the data processing power of Spark from R, and consolidating data processing, stream processing, machine learning, and graph processing into one unified and highly interoperable framework with a uniform API using Scala or Python, this book is for you. What You Will Learn Get to know the fundamentals of Spark 2 and the Spark programming model using Scala and Python Know how to use Spark SQL and DataFrames using Scala and Python Get an introduction to Spark programming using R Perform Spark data processing, charting, and plotting using Python Get acquainted with Spark stream processing using Scala and Python Be introduced to machine learning using Spark MLlib Get started with graph processing using the Spark GraphX Bring together all that you've learned and develop a complete Spark application In Detail Spark is one of the most widely-used large-scale data processing engines and runs extremely fast. It is a framework that has tools that are equally useful for application developers as well as data scientists. This book starts with the fundamentals of Spark 2 and covers the core data processing framework and API, installation, and application development setup. Then the Spark programming model is introduced through real-world examples followed by Spark SQL programming with DataFrames. An introduction to SparkR is covered next. Later, we cover the charting and plotting features of Python in conjunction with Spark data processing. After that, we take a look at Spark's stream processing, machine learning, and graph processing libraries. The last chapter combines all the skills you learned from the preceding chapters to develop a real-world Spark application. By the end of this book, you will have all the knowledge you need to develop efficient large-scale applications using Apache Spark. Style and approach Learn about Spark's infrastructure with this practical tutorial. With the help of real-world use cases on the main features of Spark we offer an easy introduction to the framework.","language":"en","currency":"USD","id":"178588669X","title":"Apache Spark 2 for Beginners","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=Q5XcDgAAQBAJ&source=gbs_api","authors":["Rajanarayanan Thottuvaikkatumana"]},{"pageCount":252,"thumbnail":"http:\/\/books.google.com\/books\/content?id=PgVwDQAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":null,"description":"Develop a range of cutting-edge machine learning projects with Apache Spark using this actionable guide About This Book Customize Apache Spark and R to fit your analytical needs in customer research, fraud detection, risk analytics, and recommendation engine development Develop a set of practical Machine Learning applications that can be implemented in real-life projects A comprehensive, project-based guide to improve and refine your predictive models for practical implementation Who This Book Is For If you are a data scientist, a data analyst, or an R and SPSS user with a good understanding of machine learning concepts, algorithms, and techniques, then this is the book for you. Some basic understanding of Spark and its core elements and application is required. What You Will Learn Set up Apache Spark for machine learning and discover its impressive processing power Combine Spark and R to unlock detailed business insights essential for decision making Build machine learning systems with Spark that can detect fraud and analyze financial risks Build predictive models focusing on customer scoring and service ranking Build a recommendation systems using SPSS on Apache Spark Tackle parallel computing and find out how it can support your machine learning projects Turn open data and communication data into actionable insights by making use of various forms of machine learning In Detail There's a reason why Apache Spark has become one of the most popular tools in Machine Learning \u2013 its ability to handle huge datasets at an impressive speed means you can be much more responsive to the data at your disposal. This book shows you Spark at its very best, demonstrating how to connect it with R and unlock maximum value not only from the tool but also from your data. Packed with a range of project \"blueprints\" that demonstrate some of the most interesting challenges that Spark can help you tackle, you'll find out how to use Spark notebooks and access, clean, and join different datasets before putting your knowledge into practice with some real-world projects, in which you will see how Spark Machine Learning can help you with everything from fraud detection to analyzing customer attrition. You'll also find out how to build a recommendation engine using Spark's parallel computing powers. Style and approach This book offers a step-by-step approach to setting up Apache Spark, and use other analytical tools with it to process Big Data and build machine learning projects.The initial chapters focus more on the theory aspect of machine learning with Spark, while each of the later chapters focuses on building standalone projects using Spark.","language":"en","currency":"USD","id":"1785887785","title":"Apache Spark Machine Learning Blueprints","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=PgVwDQAAQBAJ&source=gbs_api","authors":["Alex Liu"]},{"pageCount":354,"thumbnail":"http:\/\/books.google.com\/books\/content?id=5-ZDDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":null,"description":"Advanced analytics on your Big Data with latest Apache Spark 2.x About This Book An advanced guide with a combination of instructions and practical examples to extend the most up-to date Spark functionalities. Extend your data processing capabilities to process huge chunk of data in minimum time using advanced concepts in Spark. Master the art of real-time processing with the help of Apache Spark 2.x Who This Book Is For If you are a developer with some experience with Spark and want to strengthen your knowledge of how to get around in the world of Spark, then this book is ideal for you. Basic knowledge of Linux, Hadoop and Spark is assumed. Reasonable knowledge of Scala is expected. What You Will Learn Examine Advanced Machine Learning and DeepLearning with MLlib, SparkML, SystemML, H2O and DeepLearning4J Study highly optimised unified batch and real-time data processing using SparkSQL and Structured Streaming Evaluate large-scale Graph Processing and Analysis using GraphX and GraphFrames Apply Apache Spark in Elastic deployments using Jupyter and Zeppelin Notebooks, Docker, Kubernetes and the IBM Cloud Understand internal details of cost based optimizers used in Catalyst, SystemML and GraphFrames Learn how specific parameter settings affect overall performance of an Apache Spark cluster Leverage Scala, R and python for your data science projects In Detail Apache Spark is an in-memory cluster-based parallel processing system that provides a wide range of functionalities such as graph processing, machine learning, stream processing, and SQL. This book aims to take your knowledge of Spark to the next level by teaching you how to expand Spark's functionality and implement your data flows and machine\/deep learning programs on top of the platform. The book commences with an overview of the Spark ecosystem. It will introduce you to Project Tungsten and Catalyst, two of the major advancements of Apache Spark 2.x. You will understand how memory management and binary processing, cache-aware computation, and code generation are used to speed things up dramatically. The book extends to show how to incorporate H20, SystemML, and Deeplearning4j for machine learning, and Jupyter Notebooks and Kubernetes\/Docker for cloud-based Spark. During the course of the book, you will learn about the latest enhancements to Apache Spark 2.x, such as interactive querying of live data and unifying DataFrames and Datasets. You will also learn about the updates on the APIs and how DataFrames and Datasets affect SQL, machine learning, graph processing, and streaming. You will learn to use Spark as a big data operating system, understand how to implement advanced analytics on the new APIs, and explore how easy it is to use Spark in day-to-day tasks. Style and approach This book is an extensive guide to Apache Spark modules and tools and shows how Spark's functionality can be extended for real-time processing and storage with worked examples.","language":"en","currency":"USD","id":"178528522X","title":"Mastering Apache Spark 2.x","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=5-ZDDwAAQBAJ&source=gbs_api","authors":["Romeo Kienzler"]},{"pageCount":99998,"thumbnail":"http:\/\/books.google.com\/books\/content?id=db9aDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":null,"description":"Solve Data Analytics Problems with Spark, PySpark, and Related Open Source Tools Spark is at the heart of today\u2019s Big Data revolution, helping data professionals supercharge efficiency and performance in a wide range of data processing and analytics tasks. In this guide, Big Data expert Jeffrey Aven covers all you need to know to leverage Spark, together with its extensions, subprojects, and wider ecosystem. Aven combines a language-agnostic introduction to foundational Spark concepts with extensive programming examples utilizing the popular and intuitive PySpark development environment. This guide\u2019s focus on Python makes it widely accessible to large audiences of data professionals, analysts, and developers\u2014even those with little Hadoop or Spark experience. Aven\u2019s broad coverage ranges from basic to advanced Spark programming, and Spark SQL to machine learning. You\u2019ll learn how to efficiently manage all forms of data with Spark: streaming, structured, semi-structured, and unstructured. Throughout, concise topic overviews quickly get you up to speed, and extensive hands-on exercises prepare you to solve real problems. Coverage includes: \u2022 Understand Spark\u2019s evolving role in the Big Data and Hadoop ecosystems \u2022 Create Spark clusters using various deployment modes \u2022 Control and optimize the operation of Spark clusters and applications \u2022 Master Spark Core RDD API programming techniques \u2022 Extend, accelerate, and optimize Spark routines with advanced API platform constructs, including shared variables, RDD storage, and partitioning \u2022 Efficiently integrate Spark with both SQL and nonrelational data stores \u2022 Perform stream processing and messaging with Spark Streaming and Apache Kafka \u2022 Implement predictive modeling with SparkR and Spark MLlib","language":"en","currency":"USD","id":"0134844874","title":"Data Analytics with Spark Using Python","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=db9aDwAAQBAJ&source=gbs_api","authors":["Jeffrey Aven"]},{"pageCount":296,"thumbnail":"http:\/\/books.google.com\/books\/content?id=hZa0DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.81,"subtitle":"The Complete Guide to Large-Scale Analysis and Modeling","description":"If you\u2019re like most R users, you have deep knowledge and love for statistics. But as your organization continues to collect huge amounts of data, adding tools such as Apache Spark makes a lot of sense. With this practical book, data scientists and professionals working with large-scale data applications will learn how to use Spark from R to tackle big data and big compute problems. Authors Javier Luraschi, Kevin Kuo, and Edgar Ruiz show you how to use R with Spark to solve different data analysis problems. This book covers relevant data science topics, cluster computing, and issues that should interest even the most advanced users. Analyze, explore, transform, and visualize data in Apache Spark with R Create statistical models to extract information and predict outcomes; automate the process in production-ready workflows Perform analysis and modeling across many machines using distributed computing techniques Use large-scale data from multiple sources and different formats with ease from within Spark Learn about alternative modeling frameworks for graph processing, geospatial analysis, and genomics at scale Dive into advanced topics including custom transformations, real-time data processing, and creating custom Spark extensions","language":"en","currency":"USD","id":"1492046329","title":"Mastering Spark with R","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=hZa0DwAAQBAJ&source=gbs_api","authors":["Javier Luraschi","Kevin Kuo","Edgar Ruiz"]},{"pageCount":148,"thumbnail":"http:\/\/books.google.com\/books\/content?id=cgSKCgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":17.19,"subtitle":null,"description":"Build, process and analyze large-scale graph data effectively with Spark About This Book Find solutions for every stage of data processing from loading and transforming graph data to Improve the scalability of your graphs with a variety of real-world applications with complete Scala code. A concise guide to processing large-scale networks with Apache Spark. Who This Book Is For This book is for data scientists and big data developers who want to learn the processing and analyzing graph datasets at scale. Basic programming experience with Scala is assumed. Basic knowledge of Spark is assumed. What You Will Learn Write, build and deploy Spark applications with the Scala Build Tool. Build and analyze large-scale network datasets Analyze and transform graphs using RDD and graph-specific operations Implement new custom graph operations tailored to specific needs. Develop iterative and efficient graph algorithms using message aggregation and Pregel abstraction Extract subgraphs and use it to discover common clusters Analyze graph data and solve various data science problems using real-world datasets. In Detail Apache Spark is the next standard of open-source cluster-computing engine for processing big data. Many practical computing problems concern large graphs, like the Web graph and various social networks. The scale of these graphs - in some cases billions of vertices, trillions of edges - poses challenges to their efficient processing. Apache Spark GraphX API combines the advantages of both data-parallel and graph-parallel systems by efficiently expressing graph computation within the Spark data-parallel framework. This book will teach the user to do graphical programming in Apache Spark, apart from an explanation of the entire process of graphical data analysis. You will journey through the creation of graphs, its uses, its exploration and analysis and finally will also cover the conversion of graph elements into graph structures. This book begins with an introduction of the Spark system, its libraries and the Scala Build Tool. Using a hands-on approach, this book will quickly teach you how to install and leverage Spark interactively on the command line and in a standalone Scala program. Then, it presents all the methods for building Spark graphs using illustrative network datasets. Next, it will walk you through the process of exploring, visualizing and analyzing different network characteristics. This book will also teach you how to transform raw datasets into a usable form. In addition, you will learn powerful operations that can be used to transform graph elements and graph structures. Furthermore, this book also teaches how to create custom graph operations that are tailored for specific needs with efficiency in mind. The later chapters of this book cover more advanced topics such as clustering graphs, implementing graph-parallel iterative algorithms and learning methods from graph data. Style and approach A step-by-step guide that will walk you through the key ideas and techniques for processing big graph data at scale, with practical examples that will ensure an overall understanding of the concepts of Spark.","language":"en","currency":"USD","id":"1784398950","title":"Apache Spark Graph Processing","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=cgSKCgAAQBAJ&source=gbs_api","authors":["Rindra Ramamonjison"]},{"pageCount":311,"thumbnail":"http:\/\/books.google.com\/books\/content?id=6BHGDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":14.74,"subtitle":"Data Virtualization with SQL Server, Hadoop, Apache Spark, and Beyond","description":"Harness the power of PolyBase data virtualization software to make data from a variety of sources easily accessible through SQL queries while using the T-SQL skills you already know and have mastered. PolyBase Revealed shows you how to use the PolyBase feature of SQL Server 2019 to integrate SQL Server with Azure Blob Storage, Apache Hadoop, other SQL Server instances, Oracle, Cosmos DB, Apache Spark, and more. You will learn how PolyBase can help you reduce storage and other costs by avoiding the need for ETL processes that duplicate data in order to make it accessible from one source. PolyBase makes SQL Server into that one source, and T-SQL is your golden ticket. The book also covers PolyBase scale-out clusters, allowing you to distribute PolyBase queries among several SQL Server instances, thus improving performance. With great flexibility comes great complexity, and this book shows you where to look when queries fail, complete with coverage of internals, troubleshooting techniques, and where to find more information on obscure cross-platform errors. Data virtualization is a key target for Microsoft with SQL Server 2019. This book will help you keep your skills current, remain relevant, and build new business and career opportunities around Microsoft\u2019s product direction. What You Will Learn Install and configure PolyBase as a stand-alone service, or unlock its capabilities with a scale-out cluster Understand how PolyBase interacts with outside data sources while presenting their data as regular SQL Server tables Write queries combining data from SQL Server, Apache Hadoop, Oracle, Cosmos DB, Apache Spark, and more Troubleshoot PolyBase queries using SQL Server Dynamic Management Views Tune PolyBase queries using statistics and execution plans Solve common business problems, including \"cold storage\" of infrequently accessed data and simplifying ETL jobs Who This Book Is For SQL Server developers working in multi-platform environments who want one easy way of communicating with, and collecting data from, all of these sources","language":"en","currency":"USD","id":"1484254619","title":"PolyBase Revealed","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=6BHGDwAAQBAJ&source=gbs_api","authors":["Kevin Feasel"]},{"pageCount":294,"thumbnail":"http:\/\/books.google.com\/books\/content?id=cnc5DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":null,"description":"Over 70 recipes to help you use Apache Spark as your single big data computing platform and master its libraries About This Book This book contains recipes on how to use Apache Spark as a unified compute engine Cover how to connect various source systems to Apache Spark Covers various parts of machine learning including supervised\/unsupervised learning & recommendation engines Who This Book Is For This book is for data engineers, data scientists, and those who want to implement Spark for real-time data processing. Anyone who is using Spark (or is planning to) will benefit from this book. The book assumes you have a basic knowledge of Scala as a programming language. What You Will Learn Install and configure Apache Spark with various cluster managers & on AWS Set up a development environment for Apache Spark including Databricks Cloud notebook Find out how to operate on data in Spark with schemas Get to grips with real-time streaming analytics using Spark Streaming & Structured Streaming Master supervised learning and unsupervised learning using MLlib Build a recommendation engine using MLlib Graph processing using GraphX and GraphFrames libraries Develop a set of common applications or project types, and solutions that solve complex big data problems In Detail While Apache Spark 1.x gained a lot of traction and adoption in the early years, Spark 2.x delivers notable improvements in the areas of API, schema awareness, Performance, Structured Streaming, and simplifying building blocks to build better, faster, smarter, and more accessible big data applications. This book uncovers all these features in the form of structured recipes to analyze and mature large and complex sets of data. Starting with installing and configuring Apache Spark with various cluster managers, you will learn to set up development environments. Further on, you will be introduced to working with RDDs, DataFrames and Datasets to operate on schema aware data, and real-time streaming with various sources such as Twitter Stream and Apache Kafka. You will also work through recipes on machine learning, including supervised learning, unsupervised learning & recommendation engines in Spark. Last but not least, the final few chapters delve deeper into the concepts of graph processing using GraphX, securing your implementations, cluster optimization, and troubleshooting. Style and approach This book is packed with intuitive recipes supported with line-by-line explanations to help you understand Spark 2.x's real-time processing capabilities and deploy scalable big data solutions. This is a valuable resource for data scientists and those working on large-scale data projects.","language":"en","currency":"USD","id":"1787127516","title":"Apache Spark 2.x Cookbook","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=cnc5DwAAQBAJ&source=gbs_api","authors":["Rishi Yadav"]},{"pageCount":230,"thumbnail":"http:\/\/books.google.com\/books\/content?id=ZfdjDAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.24,"subtitle":"The Zen of Real-Time Analytics Using Apache Spark","description":"Learn the right cutting-edge skills and knowledge to leverage Spark Streaming to implement a wide array of real-time, streaming applications. This book walks you through end-to-end real-time application development using real-world applications, data, and code. Taking an application-first approach, each chapter introduces use cases from a specific industry and uses publicly available datasets from that domain to unravel the intricacies of production-grade design and implementation. The domains covered in Pro Spark Streaming include social media, the sharing economy, finance, online advertising, telecommunication, and IoT. In the last few years, Spark has become synonymous with big data processing. DStreams enhance the underlying Spark processing engine to support streaming analysis with a novel micro-batch processing model. Pro Spark Streaming by Zubair Nabi will enable you to become a specialist of latency sensitive applications by leveraging the key features of DStreams, micro-batch processing, and functional programming. To this end, the book includes ready-to-deploy examples and actual code. Pro Spark Streaming will act as the bible of Spark Streaming. What You'll Learn Discover Spark Streaming application development and best practices Work with the low-level details of discretized streams Optimize production-grade deployments of Spark Streaming via configuration recipes and instrumentation using Graphite, collectd, and Nagios Ingest data from disparate sources including MQTT, Flume, Kafka, Twitter, and a custom HTTP receiver Integrate and couple with HBase, Cassandra, and Redis Take advantage of design patterns for side-effects and maintaining state across the Spark Streaming micro-batch model Implement real-time and scalable ETL using data frames, SparkSQL, Hive, and SparkR Use streaming machine learning, predictive analytics, and recommendations Mesh batch processing with stream processing via the Lambda architecture Who This Book Is For Data scientists, big data experts, BI analysts, and data architects.","language":"en","currency":"USD","id":"148421479X","title":"Pro Spark Streaming","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=ZfdjDAAAQBAJ&source=gbs_api","authors":["Zubair Nabi"]},{"pageCount":264,"thumbnail":"http:\/\/books.google.com\/books\/content?id=6-sqDQAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":"A Guide to Apache Spark, Mesos, Akka, Cassandra, and Kafka","description":"Learn how to integrate full-stack open source big data architecture and to choose the correct technology\u2014Scala\/Spark, Mesos, Akka, Cassandra, and Kafka\u2014in every layer. Big data architecture is becoming a requirement for many different enterprises. So far, however, the focus has largely been on collecting, aggregating, and crunching large data sets in a timely manner. In many cases now, organizations need more than one paradigm to perform efficient analyses. Big Data SMACK explains each of the full-stack technologies and, more importantly, how to best integrate them. It provides detailed coverage of the practical benefits of these technologies and incorporates real-world examples in every situation. This book focuses on the problems and scenarios solved by the architecture, as well as the solutions provided by every technology. It covers the six main concepts of big data architecture and how integrate, replace, and reinforce every layer: The language: Scala The engine: Spark (SQL, MLib, Streaming, GraphX) The container: Mesos, Docker The view: Akka The storage: Cassandra The message broker: Kafka What You Will Learn: Make big data architecture without using complex Greek letter architectures Build a cheap but effective cluster infrastructure Make queries, reports, and graphs that business demands Manage and exploit unstructured and No-SQL data sources Use tools to monitor the performance of your architecture Integrate all technologies and decide which ones replace and which ones reinforce Who This Book Is For: Developers, data architects, and data scientists looking to integrate the most successful big data open stack architecture and to choose the correct technology in every layer","language":"en","currency":"USD","id":"1484221753","title":"Big Data SMACK","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=6-sqDQAAQBAJ&source=gbs_api","authors":["Raul Estrada","Isaac Ruiz"]},{"pageCount":666,"thumbnail":"http:\/\/books.google.com\/books\/content?id=0JlGDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":null,"description":"Simplify machine learning model implementations with Spark About This Book Solve the day-to-day problems of data science with Spark This unique cookbook consists of exciting and intuitive numerical recipes Optimize your work by acquiring, cleaning, analyzing, predicting, and visualizing your data Who This Book Is For This book is for Scala developers with a fairly good exposure to and understanding of machine learning techniques, but lack practical implementations with Spark. A solid knowledge of machine learning algorithms is assumed, as well as hands-on experience of implementing ML algorithms with Scala. However, you do not need to be acquainted with the Spark ML libraries and ecosystem. What You Will Learn Get to know how Scala and Spark go hand-in-hand for developers when developing ML systems with Spark Build a recommendation engine that scales with Spark Find out how to build unsupervised clustering systems to classify data in Spark Build machine learning systems with the Decision Tree and Ensemble models in Spark Deal with the curse of high-dimensionality in big data using Spark Implement Text analytics for Search Engines in Spark Streaming Machine Learning System implementation using Spark In Detail Machine learning aims to extract knowledge from data, relying on fundamental concepts in computer science, statistics, probability, and optimization. Learning about algorithms enables a wide range of applications, from everyday tasks such as product recommendations and spam filtering to cutting edge applications such as self-driving cars and personalized medicine. You will gain hands-on experience of applying these principles using Apache Spark, a resilient cluster computing system well suited for large-scale machine learning tasks. This book begins with a quick overview of setting up the necessary IDEs to facilitate the execution of code examples that will be covered in various chapters. It also highlights some key issues developers face while working with machine learning algorithms on the Spark platform. We progress by uncovering the various Spark APIs and the implementation of ML algorithms with developing classification systems, recommendation engines, text analytics, clustering, and learning systems. Toward the final chapters, we'll focus on building high-end applications and explain various unsupervised methodologies and challenges to tackle when implementing with big data ML systems. Style and approach This book is packed with intuitive recipes supported with line-by-line explanations to help you understand how to optimize your work flow and resolve problems when working with complex data modeling tasks and predictive algorithms. This is a valuable resource for data scientists and those working on large scale data projects.","language":"en","currency":"USD","id":"1782174605","title":"Apache Spark 2.x Machine Learning Cookbook","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=0JlGDwAAQBAJ&source=gbs_api","authors":["Siamak Amirghodsi","Meenakshi Rajendran","Broderick Hall","Shuen Mei"]},{"pageCount":56,"thumbnail":"http:\/\/books.google.com\/books\/content?id=6BmLCwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":0,"subtitle":null,"description":"Analytics is increasingly an integral part of day-to-day operations at today's leading businesses, and transformation is also occurring through huge growth in mobile and digital channels. Enterprise organizations are attempting to leverage analytics in new ways and transition existing analytics capabilities to respond with more flexibility while making the most efficient use of highly valuable data science skills. The recent growth and adoption of Apache Spark as an analytics framework and platform is very timely and helps meet these challenging demands. The Apache Spark environment on IBM z\/OS¬Æ and Linux on IBM z SystemsTM platforms allows this analytics framework to run on the same enterprise platform as the originating sources of data and transactions that feed it. If most of the data that will be used for Apache Spark analytics, or the most sensitive or quickly changing data is originating on z\/OS, then an Apache Spark z\/OS based environment will be the optimal choice for performance, security, and governance. This IBM¬Æ RedpaperTM publication explores the enterprise analytics market, use of Apache Spark on IBM z SystemsTM platforms, integration between Apache Spark and other enterprise data sources, and case studies and examples of what can be achieved with Apache Spark in enterprise environments. It is of interest to data scientists, data engineers, enterprise architects, or anybody looking to better understand how to combine an analytics framework and platform on enterprise systems.","language":"en","currency":"USD","id":"0738455040","title":"Apache Spark for the Enterprise: Setting the Business Free","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=6BmLCwAAQBAJ&source=gbs_api","authors":["Oliver Draese","Eberhard Hechler","Hong Min","Catherine Moxey","Pallavi Priyadarshini","Mark Simmonds","Mythili Venkatakrishnan","George Wang","IBM Redbooks"]},{"pageCount":338,"thumbnail":"http:\/\/books.google.com\/books\/content?id=syPHBgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":18.49,"subtitle":null,"description":"If you are a Scala, Java, or Python developer with an interest in machine learning and data analysis and are eager to learn how to apply common machine learning techniques at scale using the Spark framework, this is the book for you. While it may be useful to have a basic understanding of Spark, no previous experience is required.","language":"en","currency":"USD","id":"1783288523","title":"Machine Learning with Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=syPHBgAAQBAJ&source=gbs_api","authors":["Nick Pentreath"]},{"pageCount":274,"thumbnail":"http:\/\/books.google.com\/books\/content?id=HVQoDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":22.39,"subtitle":null,"description":"Build data-intensive applications locally and deploy at scale using the combined powers of Python and Spark 2.0 About This Book Learn why and how you can efficiently use Python to process data and build machine learning models in Apache Spark 2.0 Develop and deploy efficient, scalable real-time Spark solutions Take your understanding of using Spark with Python to the next level with this jump start guide Who This Book Is For If you are a Python developer who wants to learn about the Apache Spark 2.0 ecosystem, this book is for you. A firm understanding of Python is expected to get the best out of the book. Familiarity with Spark would be useful, but is not mandatory. What You Will Learn Learn about Apache Spark and the Spark 2.0 architecture Build and interact with Spark DataFrames using Spark SQL Learn how to solve graph and deep learning problems using GraphFrames and TensorFrames respectively Read, transform, and understand data and use it to train machine learning models Build machine learning models with MLlib and ML Learn how to submit your applications programmatically using spark-submit Deploy locally built applications to a cluster In Detail Apache Spark is an open source framework for efficient cluster computing with a strong interface for data parallelism and fault tolerance. This book will show you how to leverage the power of Python and put it to use in the Spark ecosystem. You will start by getting a firm understanding of the Spark 2.0 architecture and how to set up a Python environment for Spark. You will get familiar with the modules available in PySpark. You will learn how to abstract data with RDDs and DataFrames and understand the streaming capabilities of PySpark. Also, you will get a thorough overview of machine learning capabilities of PySpark using ML and MLlib, graph processing using GraphFrames, and polyglot persistence using Blaze. Finally, you will learn how to deploy your applications to the cloud using the spark-submit command. By the end of this book, you will have established a firm understanding of the Spark Python API and how it can be used to build data-intensive applications. Style and approach This book takes a very comprehensive, step-by-step approach so you understand how the Spark ecosystem can be used with Python to develop efficient, scalable solutions. Every chapter is standalone and written in a very easy-to-understand manner, with a focus on both the hows and the whys of each concept.","language":"en","currency":"USD","id":"1786466252","title":"Learning PySpark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=HVQoDwAAQBAJ&source=gbs_api","authors":["Tomasz Drabas","Denny Lee"]},{"pageCount":330,"thumbnail":"http:\/\/books.google.com\/books\/content?id=58RiDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":"Over 60 recipes for implementing big data processing and analytics using Apache Spark and Python","description":"Combine the power of Apache Spark and Python to build effective big data applications Key Features Perform effective data processing, machine learning, and analytics using PySpark Overcome challenges in developing and deploying Spark solutions using Python Explore recipes for efficiently combining Python and Apache Spark to process data Book Description Apache Spark is an open source framework for efficient cluster computing with a strong interface for data parallelism and fault tolerance. The PySpark Cookbook presents effective and time-saving recipes for leveraging the power of Python and putting it to use in the Spark ecosystem. You\u2019ll start by learning the Apache Spark architecture and how to set up a Python environment for Spark. You\u2019ll then get familiar with the modules available in PySpark and start using them effortlessly. In addition to this, you\u2019ll discover how to abstract data with RDDs and DataFrames, and understand the streaming capabilities of PySpark. You\u2019ll then move on to using ML and MLlib in order to solve any problems related to the machine learning capabilities of PySpark and use GraphFrames to solve graph-processing problems. Finally, you will explore how to deploy your applications to the cloud using the spark-submit command. By the end of this book, you will be able to use the Python API for Apache Spark to solve any problems associated with building data-intensive applications. What you will learn Configure a local instance of PySpark in a virtual environment Install and configure Jupyter in local and multi-node environments Create DataFrames from JSON and a dictionary using pyspark.sql Explore regression and clustering models available in the ML module Use DataFrames to transform data used for modeling Connect to PubNub and perform aggregations on streams Who this book is for The PySpark Cookbook is for you if you are a Python developer looking for hands-on recipes for using the Apache Spark 2.x ecosystem in the best possible way. A thorough understanding of Python (and some familiarity with Spark) will help you get the best out of the book.","language":"en","currency":"USD","id":"1788834259","title":"PySpark Cookbook","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=58RiDwAAQBAJ&source=gbs_api","authors":["Denny Lee","Tomasz Drabas"]},{"pageCount":557,"thumbnail":"http:\/\/books.google.com\/books\/content?id=teZfDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":"A Practical Guide to Apache Kudu, Impala, and Spark","description":"Utilize this practical and easy-to-follow guide to modernize traditional enterprise data warehouse and business intelligence environments with next-generation big data technologies. Next-Generation Big Data takes a holistic approach, covering the most important aspects of modern enterprise big data. The book covers not only the main technology stack but also the next-generation tools and applications used for big data warehousing, data warehouse optimization, real-time and batch data ingestion and processing, real-time data visualization, big data governance, data wrangling, big data cloud deployments, and distributed in-memory big data computing. Finally, the book has an extensive and detailed coverage of big data case studies from Navistar, Cerner, British Telecom, Shopzilla, Thomson Reuters, and Mastercard. What You\u2019ll Learn Install Apache Kudu, Impala, and Spark to modernize enterprise data warehouse and business intelligence environments, complete with real-world, easy-to-follow examples, and practical advice Integrate HBase, Solr, Oracle, SQL Server, MySQL, Flume, Kafka, HDFS, and Amazon S3 with Apache Kudu, Impala, and Spark Use StreamSets, Talend, Pentaho, and CDAP for real-time and batch data ingestion and processing Utilize Trifacta, Alteryx, and Datameer for data wrangling and interactive data processing Turbocharge Spark with Alluxio, a distributed in-memory storage platform Deploy big data in the cloud using Cloudera Director Perform real-time data visualization and time series analysis using Zoomdata, Apache Kudu, Impala, and Spark Understand enterprise big data topics such as big data governance, metadata management, data lineage, impact analysis, and policy enforcement, and how to use Cloudera Navigator to perform common data governance tasks Implement big data use cases such as big data warehousing, data warehouse optimization, Internet of Things, real-time data ingestion and analytics, complex event processing, and scalable predictive modeling Study real-world big data case studies from innovative companies, including Navistar, Cerner, British Telecom, Shopzilla, Thomson Reuters, and Mastercard Who This Book Is For BI and big data warehouse professionals interested in gaining practical and real-world insight into next-generation big data processing and analytics using Apache Kudu, Impala, and Spark; and those who want to learn more about other advanced enterprise topics","language":"en","currency":"USD","id":"1484231473","title":"Next-Generation Big Data","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=teZfDwAAQBAJ&source=gbs_api","authors":["Butch Quinto"]},{"pageCount":848,"thumbnail":"http:\/\/books.google.com\/books\/content?id=oKiLDQAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":"Managing Spark, YARN, and MapReduce","description":"This is the eBook of the printed book and may not include any media, website access codes, or print supplements that may come packaged with the bound book. The Comprehensive, Up-to-Date Apache Hadoop Administration Handbook and Reference \u201CSam Alapati has worked with production Hadoop clusters for six years. His unique depth of experience has enabled him to write the go-to resource for all administrators looking to spec, size, expand, and secure production Hadoop clusters of any size.\u201D \u2014Paul Dix, Series Editor In Expert Hadoop¬Æ Administration, leading Hadoop administrator Sam R. Alapati brings together authoritative knowledge for creating, configuring, securing, managing, and optimizing production Hadoop clusters in any environment. Drawing on his experience with large-scale Hadoop administration, Alapati integrates action-oriented advice with carefully researched explanations of both problems and solutions. He covers an unmatched range of topics and offers an unparalleled collection of realistic examples. Alapati demystifies complex Hadoop environments, helping you understand exactly what happens behind the scenes when you administer your cluster. You\u2019ll gain unprecedented insight as you walk through building clusters from scratch and configuring high availability, performance, security, encryption, and other key attributes. The high-value administration skills you learn here will be indispensable no matter what Hadoop distribution you use or what Hadoop applications you run. Understand Hadoop\u2019s architecture from an administrator\u2019s standpoint Create simple and fully distributed clusters Run MapReduce and Spark applications in a Hadoop cluster Manage and protect Hadoop data and high availability Work with HDFS commands, file permissions, and storage management Move data, and use YARN to allocate resources and schedule jobs Manage job workflows with Oozie and Hue Secure, monitor, log, and optimize Hadoop Benchmark and troubleshoot Hadoop","language":"en","currency":"USD","id":"0134703383","title":"Expert Hadoop 2 Administration","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=oKiLDQAAQBAJ&source=gbs_api","authors":["Sam R. Alapati"]},{"pageCount":392,"thumbnail":"http:\/\/books.google.com\/books\/content?id=hdDcDgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":22.39,"subtitle":null,"description":"Over insightful 90 recipes to get lightning-fast analytics with Apache Spark About This Book Use Apache Spark for data processing with these hands-on recipes Implement end-to-end, large-scale data analysis better than ever before Work with powerful libraries such as MLLib, SciPy, NumPy, and Pandas to gain insights from your data Who This Book Is For This book is for novice and intermediate level data science professionals and data analysts who want to solve data science problems with a distributed computing framework. Basic experience with data science implementation tasks is expected. Data science professionals looking to skill up and gain an edge in the field will find this book helpful. What You Will Learn Explore the topics of data mining, text mining, Natural Language Processing, information retrieval, and machine learning. Solve real-world analytical problems with large data sets. Address data science challenges with analytical tools on a distributed system like Spark (apt for iterative algorithms), which offers in-memory processing and more flexibility for data analysis at scale. Get hands-on experience with algorithms like Classification, regression, and recommendation on real datasets using Spark MLLib package. Learn about numerical and scientific computing using NumPy and SciPy on Spark. Use Predictive Model Markup Language (PMML) in Spark for statistical data mining models. In Detail Spark has emerged as the most promising big data analytics engine for data science professionals. The true power and value of Apache Spark lies in its ability to execute data science tasks with speed and accuracy. Spark's selling point is that it combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. It lets you tackle the complexities that come with raw unstructured data sets with ease. This guide will get you comfortable and confident performing data science tasks with Spark. You will learn about implementations including distributed deep learning, numerical computing, and scalable machine learning. You will be shown effective solutions to problematic concepts in data science using Spark's data science libraries such as MLLib, Pandas, NumPy, SciPy, and more. These simple and efficient recipes will show you how to implement algorithms and optimize your work. Style and approach This book contains a comprehensive range of recipes designed to help you learn the fundamentals and tackle the difficulties of data science. This book outlines practical steps to produce powerful insights into Big Data through a recipe-based approach.","language":"en","currency":"USD","id":"1785288806","title":"Apache Spark for Data Science Cookbook","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=hdDcDgAAQBAJ&source=gbs_api","authors":["Padma Priya Chitturi"]},{"pageCount":532,"thumbnail":"http:\/\/books.google.com\/books\/content?id=LUIwDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":null,"description":"Create scalable machine learning applications to power a modern data-driven business using Spark 2.x About This Book Get to the grips with the latest version of Apache Spark Utilize Spark's machine learning library to implement predictive analytics Leverage Spark's powerful tools to load, analyze, clean, and transform your data Who This Book Is For If you have a basic knowledge of machine learning and want to implement various machine-learning concepts in the context of Spark ML, this book is for you. You should be well versed with the Scala and Python languages. What You Will Learn Get hands-on with the latest version of Spark ML Create your first Spark program with Scala and Python Set up and configure a development environment for Spark on your own computer, as well as on Amazon EC2 Access public machine learning datasets and use Spark to load, process, clean, and transform data Use Spark's machine learning library to implement programs by utilizing well-known machine learning models Deal with large-scale text data, including feature extraction and using text data as input to your machine learning models Write Spark functions to evaluate the performance of your machine learning models In Detail This book will teach you about popular machine learning algorithms and their implementation. You will learn how various machine learning concepts are implemented in the context of Spark ML. You will start by installing Spark in a single and multinode cluster. Next you'll see how to execute Scala and Python based programs for Spark ML. Then we will take a few datasets and go deeper into clustering, classification, and regression. Toward the end, we will also cover text processing using Spark ML. Once you have learned the concepts, they can be applied to implement algorithms in either green-field implementations or to migrate existing systems to this new platform. You can migrate from Mahout or Scikit to use Spark ML. By the end of this book, you will acquire the skills to leverage Spark's features to create your own scalable machine learning applications and power a modern data-driven business. Style and approach This practical tutorial with real-world use cases enables you to develop your own machine learning systems with Spark. The examples will help you combine various techniques and models into an intelligent machine learning system.","language":"en","currency":"USD","id":"1785886428","title":"Machine Learning with Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=LUIwDwAAQBAJ&source=gbs_api","authors":["Rajdeep Dua","Manpreet Singh Ghotra","Nick Pentreath"]},{"pageCount":452,"thumbnail":"http:\/\/books.google.com\/books\/content?id=8JlGDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.72,"subtitle":null,"description":"Design, implement, and deliver successful streaming applications, machine learning pipelines and graph applications using Spark SQL API About This Book Learn about the design and implementation of streaming applications, machine learning pipelines, deep learning, and large-scale graph processing applications using Spark SQL APIs and Scala. Learn data exploration, data munging, and how to process structured and semi-structured data using real-world datasets and gain hands-on exposure to the issues and challenges of working with noisy and \"dirty\" real-world data. Understand design considerations for scalability and performance in web-scale Spark application architectures. Who This Book Is For If you are a developer, engineer, or an architect and want to learn how to use Apache Spark in a web-scale project, then this is the book for you. It is assumed that you have prior knowledge of SQL querying. A basic programming knowledge with Scala, Java, R, or Python is all you need to get started with this book. What You Will Learn Familiarize yourself with Spark SQL programming, including working with DataFrame\/Dataset API and SQL Perform a series of hands-on exercises with different types of data sources, including CSV, JSON, Avro, MySQL, and MongoDB Perform data quality checks, data visualization, and basic statistical analysis tasks Perform data munging tasks on publically available datasets Learn how to use Spark SQL and Apache Kafka to build streaming applications Learn key performance-tuning tips and tricks in Spark SQL applications Learn key architectural components and patterns in large-scale Spark SQL applications In Detail In the past year, Apache Spark has been increasingly adopted for the development of distributed applications. Spark SQL APIs provide an optimized interface that helps developers build such applications quickly and easily. However, designing web-scale production applications using Spark SQL APIs can be a complex task. Hence, understanding the design and implementation best practices before you start your project will help you avoid these problems. This book gives an insight into the engineering practices used to design and build real-world, Spark-based applications. The book's hands-on examples will give you the required confidence to work on any future projects you encounter in Spark SQL. It starts by familiarizing you with data exploration and data munging tasks using Spark SQL and Scala. Extensive code examples will help you understand the methods used to implement typical use-cases for various types of applications. You will get a walkthrough of the key concepts and terms that are common to streaming, machine learning, and graph applications. You will also learn key performance-tuning details including Cost Based Optimization (Spark 2.2) in Spark SQL applications. Finally, you will move on to learning how such systems are architected and deployed for a successful delivery of your project. Style and approach This book is a hands-on guide to designing, building, and deploying Spark SQL-centric production applications at scale.","language":"en","currency":"USD","id":"1785887351","title":"Learning Spark SQL","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=8JlGDwAAQBAJ&source=gbs_api","authors":["Aurobindo Sarkar"]},{"pageCount":532,"thumbnail":"http:\/\/books.google.com\/books\/content?id=LUIwDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":null,"description":"Create scalable machine learning applications to power a modern data-driven business using Spark 2.x About This Book Get to the grips with the latest version of Apache Spark Utilize Spark's machine learning library to implement predictive analytics Leverage Spark's powerful tools to load, analyze, clean, and transform your data Who This Book Is For If you have a basic knowledge of machine learning and want to implement various machine-learning concepts in the context of Spark ML, this book is for you. You should be well versed with the Scala and Python languages. What You Will Learn Get hands-on with the latest version of Spark ML Create your first Spark program with Scala and Python Set up and configure a development environment for Spark on your own computer, as well as on Amazon EC2 Access public machine learning datasets and use Spark to load, process, clean, and transform data Use Spark's machine learning library to implement programs by utilizing well-known machine learning models Deal with large-scale text data, including feature extraction and using text data as input to your machine learning models Write Spark functions to evaluate the performance of your machine learning models In Detail This book will teach you about popular machine learning algorithms and their implementation. You will learn how various machine learning concepts are implemented in the context of Spark ML. You will start by installing Spark in a single and multinode cluster. Next you'll see how to execute Scala and Python based programs for Spark ML. Then we will take a few datasets and go deeper into clustering, classification, and regression. Toward the end, we will also cover text processing using Spark ML. Once you have learned the concepts, they can be applied to implement algorithms in either green-field implementations or to migrate existing systems to this new platform. You can migrate from Mahout or Scikit to use Spark ML. By the end of this book, you will acquire the skills to leverage Spark's features to create your own scalable machine learning applications and power a modern data-driven business. Style and approach This practical tutorial with real-world use cases enables you to develop your own machine learning systems with Spark. The examples will help you combine various techniques and models into an intelligent machine learning system.","language":"en","currency":"USD","id":"1785886428","title":"Machine Learning with Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=LUIwDwAAQBAJ&source=gbs_api","authors":["Rajdeep Dua","Manpreet Singh Ghotra","Nick Pentreath"]},{"pageCount":274,"thumbnail":"http:\/\/books.google.com\/books\/content?id=MJvcDgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":null,"description":"Learn how to use Spark to process big data at speed and scale for sharper analytics. Put the principles into practice for faster, slicker big data projects. About This Book A quick way to get started with Spark \u2013 and reap the rewards From analytics to engineering your big data architecture, we've got it covered Bring your Scala and Java knowledge \u2013 and put it to work on new and exciting problems Who This Book Is For This book is for developers with little to no knowledge of Spark, but with a background in Scala\/Java programming. It's recommended that you have experience in dealing and working with big data and a strong interest in data science. What You Will Learn Install and set up Spark in your cluster Prototype distributed applications with Spark's interactive shell Perform data wrangling using the new DataFrame APIs Get to know the different ways to interact with Spark's distributed representation of data (RDDs) Query Spark with a SQL-like query syntax See how Spark works with big data Implement machine learning systems with highly scalable algorithms Use R, the popular statistical language, to work with Spark Apply interesting graph algorithms and graph processing with GraphX In Detail When people want a way to process big data at speed, Spark is invariably the solution. With its ease of development (in comparison to the relative complexity of Hadoop), it's unsurprising that it's becoming popular with data analysts and engineers everywhere. Beginning with the fundamentals, we'll show you how to get set up with Spark with minimum fuss. You'll then get to grips with some simple APIs before investigating machine learning and graph processing \u2013 throughout we'll make sure you know exactly how to apply your knowledge. You will also learn how to use the Spark shell, how to load data before finding out how to build and run your own Spark applications. Discover how to manipulate your RDD and get stuck into a range of DataFrame APIs. As if that's not enough, you'll also learn some useful Machine Learning algorithms with the help of Spark MLlib and integrating Spark with R. We'll also make sure you're confident and prepared for graph processing, as you learn more about the GraphX API. Style and approach This book is a basic, step-by-step tutorial that will help you take advantage of all that Spark has to offer.","language":"en","currency":"USD","id":"1785882961","title":"Fast Data Processing with Spark 2","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=MJvcDgAAQBAJ&source=gbs_api","authors":["Krishna Sankar"]},{"pageCount":216,"thumbnail":"http:\/\/books.google.com\/books\/content?id=SDa7CwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":40,"subtitle":"Big Data Cluster Computing in Production","description":"Production-targeted Spark guidance with real-world use cases Spark: Big Data Cluster Computing in Production goes beyond general Spark overviews to provide targeted guidance toward using lightning-fast big-data clustering in production. Written by an expert team well-known in the big data community, this book walks you through the challenges in moving from proof-of-concept or demo Spark applications to live Spark in production. Real use cases provide deep insight into common problems, limitations, challenges, and opportunities, while expert tips and tricks help you get the most out of Spark performance. Coverage includes Spark SQL, Tachyon, Kerberos, ML Lib, YARN, and Mesos, with clear, actionable guidance on resource scheduling, db connectors, streaming, security, and much more. Spark has become the tool of choice for many Big Data problems, with more active contributors than any other Apache Software project. General introductory books abound, but this book is the first to provide deep insight and real-world advice on using Spark in production. Specific guidance, expert tips, and invaluable foresight make this guide an incredibly useful resource for real production settings. Review Spark hardware requirements and estimate cluster size Gain insight from real-world production use cases Tighten security, schedule resources, and fine-tune performance Overcome common problems encountered using Spark in production Spark works with other big data tools including MapReduce and Hadoop, and uses languages you already know like Java, Scala, Python, and R. Lightning speed makes Spark too good to pass up, but understanding limitations and challenges in advance goes a long way toward easing actual production implementation. Spark: Big Data Cluster Computing in Production tells you everything you need to know, with real-world production insight and expert guidance, tips, and tricks.","language":"en","currency":"USD","id":"1119254043","title":"Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=SDa7CwAAQBAJ&source=gbs_api","authors":["Ilya Ganelin","Ema Orhian","Kai Sasaki","Brennon York"]},{"pageCount":182,"thumbnail":"http:\/\/books.google.com\/books\/content?id=jc-PDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":11.99,"subtitle":"Analyze large datasets and discover techniques for testing, immunizing, and parallelizing Spark jobs","description":"Use PySpark to easily crush messy data at-scale and discover proven techniques to create testable, immutable, and easily parallelizable Spark jobs Key Features Work with large amounts of agile data using distributed datasets and in-memory caching Source data from all popular data hosting platforms, such as HDFS, Hive, JSON, and S3 Employ the easy-to-use PySpark API to deploy big data Analytics for production Book Description Apache Spark is an open source parallel-processing framework that has been around for quite some time now. One of the many uses of Apache Spark is for data analytics applications across clustered computers. In this book, you will not only learn how to use Spark and the Python API to create high-performance analytics with big data, but also discover techniques for testing, immunizing, and parallelizing Spark jobs. You will learn how to source data from all popular data hosting platforms, including HDFS, Hive, JSON, and S3, and deal with large datasets with PySpark to gain practical big data experience. This book will help you work on prototypes on local machines and subsequently go on to handle messy data in production and at scale. This book covers installing and setting up PySpark, RDD operations, big data cleaning and wrangling, and aggregating and summarizing data into useful reports. You will also learn how to implement some practical and proven techniques to improve certain aspects of programming and administration in Apache Spark. By the end of the book, you will be able to build big data analytical solutions using the various PySpark offerings and also optimize them effectively. What you will learn Get practical big data experience while working on messy datasets Analyze patterns with Spark SQL to improve your business intelligence Use PySpark's interactive shell to speed up development time Create highly concurrent Spark programs by leveraging immutability Discover ways to avoid the most expensive operation in the Spark API: the shuffle operation Re-design your jobs to use reduceByKey instead of groupBy Create robust processing pipelines by testing Apache Spark jobs Who this book is for This book is for developers, data scientists, business analysts, or anyone who needs to reliably analyze large amounts of large-scale, real-world data. Whether you're tasked with creating your company's business intelligence function or creating great data platforms for your machine learning models, or are looking to use code to magnify the impact of your business, this book is for you.","language":"en","currency":"USD","id":"1838648836","title":"Hands-On Big Data Analytics with PySpark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=jc-PDwAAQBAJ&source=gbs_api","authors":["Rudy Lai","Bart≈Çomiej Potaczek"]},{"pageCount":142,"thumbnail":"http:\/\/books.google.com\/books\/content?id=ArrVDAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":0,"subtitle":null,"description":"The term big data refers to extremely large sets of data that are analyzed to reveal insights, such as patterns, trends, and associations. The algorithms that analyze this data to provide these insights must extract value from a wide range of data sources, including business data and live, streaming, social media data. However, the real value of these insights comes from their timeliness. Rapid delivery of insights enables anyone (not only data scientists) to make effective decisions, applying deep intelligence to every enterprise application. Apache Spark is an integrated analytics framework and runtime to accelerate and simplify algorithm development, depoyment, and realization of business insight from analytics. Apache Spark on IBM¬Æ z\/OS¬Æ puts the open source engine, augmented with unique differentiated features, built specifically for data science, where big data resides. This IBM Redbooks¬Æ publication describes the installation and configuration of IBM z\/OS Platform for Apache Spark for field teams and clients. Additionally, it includes examples of business analytics scenarios.","language":"en","currency":"USD","id":"0738414964","title":"Apache Spark Implementation on IBM z\/OS","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=ArrVDAAAQBAJ&source=gbs_api","authors":["Lydia Parziale","Joe Bostian","Ravi Kumar","Ulrich Seelbach","Zhong Yu Ye","IBM Redbooks"]},{"pageCount":778,"thumbnail":"http:\/\/books.google.com\/books\/content?id=10AoCgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":47.39,"subtitle":"Recipes for Scaling Up with Hadoop and Spark","description":"If you are ready to dive into the MapReduce framework for processing large datasets, this practical book takes you step by step through the algorithms and tools you need to build distributed MapReduce applications with Apache Hadoop or Apache Spark. Each chapter provides a recipe for solving a massive computational problem, such as building a recommendation system. You\u2019ll learn how to implement the appropriate MapReduce solution with code that you can use in your projects. Dr. Mahmoud Parsian covers basic design patterns, optimization techniques, and data mining and machine learning solutions for problems in bioinformatics, genomics, statistics, and social network analysis. This book also includes an overview of MapReduce, Hadoop, and Spark. Topics include: Market basket analysis for a large set of transactions Data mining algorithms (K-means, KNN, and Naive Bayes) Using huge genomic data to sequence DNA and RNA Naive Bayes theorem and Markov chains for data and market prediction Recommendation algorithms and pairwise document similarity Linear regression, Cox regression, and Pearson correlation Allelic frequency and mining DNA Social network analysis (recommendation systems, counting triangles, sentiment analysis)","language":"en","currency":"USD","id":"1491906138","title":"Data Algorithms","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=10AoCgAAQBAJ&source=gbs_api","authors":["Mahmoud Parsian"]},{"pageCount":344,"thumbnail":"http:\/\/books.google.com\/books\/content?id=hIRcDgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":null,"description":"Analyze your data and delve deep into the world of machine learning with the latest Spark version, 2.0 About This Book Perform data analysis and build predictive models on huge datasets that leverage Apache Spark Learn to integrate data science algorithms and techniques with the fast and scalable computing features of Spark to address big data challenges Work through practical examples on real-world problems with sample code snippets Who This Book Is For This book is for anyone who wants to leverage Apache Spark for data science and machine learning. If you are a technologist who wants to expand your knowledge to perform data science operations in Spark, or a data scientist who wants to understand how algorithms are implemented in Spark, or a newbie with minimal development experience who wants to learn about Big Data Analytics, this book is for you! What You Will Learn Consolidate, clean, and transform your data acquired from various data sources Perform statistical analysis of data to find hidden insights Explore graphical techniques to see what your data looks like Use machine learning techniques to build predictive models Build scalable data products and solutions Start programming using the RDD, DataFrame and Dataset APIs Become an expert by improving your data analytical skills In Detail This is the era of Big Data. The words “Çig Data' implies big innovation and enables a competitive advantage for businesses. Apache Spark was designed to perform Big Data analytics at scale, and so Spark is equipped with the necessary algorithms and supports multiple programming languages. Whether you are a technologist, a data scientist, or a beginner to Big Data analytics, this book will provide you with all the skills necessary to perform statistical data analysis, data visualization, predictive modeling, and build scalable data products or solutions using Python, Scala, and R. With ample case studies and real-world examples, Spark for Data Science will help you ensure the successful execution of your data science projects. Style and approach This book takes a step-by-step approach to statistical analysis and machine learning, and is explained in a conversational and easy-to-follow style. Each topic is explained sequentially with a focus on the fundamentals as well as the advanced concepts of algorithms and techniques. Real-world examples with sample code snippets are also included.","language":"en","currency":"USD","id":"1785884778","title":"Spark for Data Science","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=hIRcDgAAQBAJ&source=gbs_api","authors":["Srinivas Duvvuri","Bikramaditya Singhal"]},{"pageCount":382,"thumbnail":"http:\/\/books.google.com\/books\/content?id=axOxBgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":18.49,"subtitle":null,"description":"If you are a system or application developer interested in learning how to solve practical problems using the Hadoop framework, then this book is ideal for you. You are expected to be familiar with the Unix\/Linux command-line interface and have some experience with the Java programming language. Familiarity with Hadoop would be a plus.","language":"en","currency":"USD","id":"1783285524","title":"Learning Hadoop 2","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=axOxBgAAQBAJ&source=gbs_api","authors":["Garry Turkington","Gabriele Modena"]},{"pageCount":130,"thumbnail":"http:\/\/books.google.com\/books\/content?id=UC_WBgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":10.69,"subtitle":null,"description":"If you are a data scientist who has some experience with the Hadoop ecosystem and machine learning methods and want to try out classification on large datasets using Mahout, this book is ideal for you. Knowledge of Java is essential.","language":"en","currency":"USD","id":"1783554967","title":"Learning Apache Mahout Classification","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=UC_WBgAAQBAJ&source=gbs_api","authors":["Ashish Gupta"]},{"pageCount":520,"thumbnail":"http:\/\/books.google.com\/books\/content?id=d5EIBgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":22.39,"subtitle":null,"description":"Are you curious about AI? All you need is a good understanding of the Scala programming language, a basic knowledge of statistics, a keen interest in Big Data processing, and this book!","language":"en","currency":"USD","id":"178355875X","title":"Scala for Machine Learning","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=d5EIBgAAQBAJ&source=gbs_api","authors":["Patrick R. Nicolas"]},{"pageCount":322,"thumbnail":"http:\/\/books.google.com\/books\/content?id=dXwzDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":37.12,"subtitle":"Real-Time Data and Stream Processing at Scale","description":"Every enterprise application creates data, whether it\u2019s log messages, metrics, user activity, outgoing messages, or something else. And how to move all of this data becomes nearly as important as the data itself. If you\u2019re an application architect, developer, or production engineer new to Apache Kafka, this practical guide shows you how to use this open source streaming platform to handle real-time data feeds. Engineers from Confluent and LinkedIn who are responsible for developing Kafka explain how to deploy production Kafka clusters, write reliable event-driven microservices, and build scalable stream-processing applications with this platform. Through detailed examples, you\u2019ll learn Kafka\u2019s design principles, reliability guarantees, key APIs, and architecture details, including the replication protocol, the controller, and the storage layer. Understand publish-subscribe messaging and how it fits in the big data ecosystem. Explore Kafka producers and consumers for writing and reading messages Understand Kafka patterns and use-case requirements to ensure reliable data delivery Get best practices for building data pipelines and applications with Kafka Manage Kafka in production, and learn to perform monitoring, tuning, and maintenance tasks Learn the most critical metrics among Kafka\u2019s operational measurements Explore how Kafka\u2019s stream delivery capabilities make it a perfect source for stream processing systems","language":"en","currency":"USD","id":"1491936118","title":"Kafka: The Definitive Guide","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=dXwzDwAAQBAJ&source=gbs_api","authors":["Neha Narkhede","Gwen Shapira","Todd Palino"]},{"pageCount":352,"thumbnail":"http:\/\/books.google.com\/books\/content?id=TAxlDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.81,"subtitle":"The What, Where, When, and How of Large-Scale Data Processing","description":"Streaming data is a big deal in big data these days. As more and more businesses seek to tame the massive unbounded data sets that pervade our world, streaming systems have finally reached a level of maturity sufficient for mainstream adoption. With this practical guide, data engineers, data scientists, and developers will learn how to work with streaming data in a conceptual and platform-agnostic way. Expanded from Tyler Akidau\u2019s popular blog posts \"Streaming 101\" and \"Streaming 102\", this book takes you from an introductory level to a nuanced understanding of the what, where, when, and how of processing real-time data streams. You\u2019ll also dive deep into watermarks and exactly-once processing with co-authors Slava Chernyak and Reuven Lax. You\u2019ll explore: How streaming and batch data processing patterns compare The core principles and concepts behind robust out-of-order data processing How watermarks track progress and completeness in infinite datasets How exactly-once data processing techniques ensure correctness How the concepts of streams and tables form the foundations of both batch and streaming data processing The practical motivations behind a powerful persistent state mechanism, driven by a real-world example How time-varying relations provide a link between stream processing and the world of SQL and relational algebra","language":"en","currency":"USD","id":"1491983825","title":"Streaming Systems","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=TAxlDwAAQBAJ&source=gbs_api","authors":["Tyler Akidau","Slava Chernyak","Reuven Lax"]},{"pageCount":278,"thumbnail":"http:\/\/books.google.com\/books\/content?id=ZpZGDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":22.39,"subtitle":null,"description":"Design and administer fast, reliable enterprise messaging systems with Apache Kafka About This Book Build efficient real-time streaming applications in Apache Kafka to process data streams of data Master the core Kafka APIs to set up Apache Kafka clusters and start writing message producers and consumers A comprehensive guide to help you get a solid grasp of the Apache Kafka concepts in Apache Kafka with pracitcalpractical examples Who This Book Is For If you want to learn how to use Apache Kafka and the different tools in the Kafka ecosystem in the easiest possible manner, this book is for you. Some programming experience with Java is required to get the most out of this book What You Will Learn Learn the basics of Apache Kafka from scratch Use the basic building blocks of a streaming application Design effective streaming applications with Kafka using Spark, Storm &, and Heron Understand the importance of a low -latency , high- throughput, and fault-tolerant messaging system Make effective capacity planning while deploying your Kafka Application Understand and implement the best security practices In Detail Apache Kafka is a popular distributed streaming platform that acts as a messaging queue or an enterprise messaging system. It lets you publish and subscribe to a stream of records, and process them in a fault-tolerant way as they occur. This book is a comprehensive guide to designing and architecting enterprise-grade streaming applications using Apache Kafka and other big data tools. It includes best practices for building such applications, and tackles some common challenges such as how to use Kafka efficiently and handle high data volumes with ease. This book first takes you through understanding the type messaging system and then provides a thorough introduction to Apache Kafka and its internal details. The second part of the book takes you through designing streaming application using various frameworks and tools such as Apache Spark, Apache Storm, and more. Once you grasp the basics, we will take you through more advanced concepts in Apache Kafka such as capacity planning and security. By the end of this book, you will have all the information you need to be comfortable with using Apache Kafka, and to design efficient streaming data applications with it. Style and approach A step-by \u2013step, comprehensive guide filled with practical and real- world examples","language":"en","currency":"USD","id":"1787287637","title":"Building Data Streaming Applications with Apache Kafka","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=ZpZGDwAAQBAJ&source=gbs_api","authors":["Manish Kumar","Chanchal Singh"]},{"pageCount":176,"thumbnail":"http:\/\/books.google.com\/books\/content?id=DrfNBgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":10.69,"subtitle":null,"description":"If you have a working knowledge of Hadoop 1.x but want to start afresh with YARN, this book is ideal for you. You will be able to install and administer a YARN cluster and also discover the configuration settings to fine-tune your cluster both in terms of performance and scalability. This book will help you develop, deploy, and run multiple applications\/frameworks on the same shared YARN cluster.","language":"en","currency":"USD","id":"1784397725","title":"YARN Essentials","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=DrfNBgAAQBAJ&source=gbs_api","authors":["Amol Fasale","Nirmal Kumar"]},{"pageCount":400,"thumbnail":"http:\/\/books.google.com\/books\/content?id=Bkb1DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":47.39,"subtitle":null,"description":"Data is bigger, arrives faster, and comes in a variety of formats\u2014and it all needs to be processed at scale for analytics or machine learning. But how can you process such varied workloads efficiently? Enter Apache Spark. Updated to include Spark 3.0, this second edition shows data engineers and data scientists why structure and unification in Spark matters. Specifically, this book explains how to perform simple and complex data analytics and employ machine learning algorithms. Through step-by-step walk-throughs, code snippets, and notebooks, you\u2019ll be able to: Learn Python, SQL, Scala, or Java high-level Structured APIs Understand Spark operations and SQL Engine Inspect, tune, and debug Spark operations with Spark configurations and Spark UI Connect to data sources: JSON, Parquet, CSV, Avro, ORC, Hive, S3, or Kafka Perform analytics on batch and streaming data using Structured Streaming Build reliable data pipelines with open source Delta Lake and Spark Develop machine learning pipelines with MLlib and productionize models using MLflow","language":"en","currency":"USD","id":"1492049999","title":"Learning Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=Bkb1DwAAQBAJ&source=gbs_api","authors":["Jules S. Damji","Brooke Wenig","Tathagata Das","Denny Lee"]},{"pageCount":208,"thumbnail":"http:\/\/books.google.com\/books\/content?id=4S7WBgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":14.59,"subtitle":null,"description":"If you are a data analyst, developer, or simply someone who wants to use Hive to explore and analyze data in Hadoop, this is the book for you. Whether you are new to big data or an expert, with this book, you will be able to master both the basic and the advanced features of Hive. Since Hive is an SQL-like language, some previous experience with the SQL language and databases is useful to have a better understanding of this book.","language":"en","currency":"USD","id":"1782175059","title":"Apache Hive Essentials","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=4S7WBgAAQBAJ&source=gbs_api","authors":["Dayong Du"]},{"pageCount":264,"thumbnail":"http:\/\/books.google.com\/books\/content?id=wzFgDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":111.2,"subtitle":null,"description":"The book describes the emergence of big data technologies and the role of Spark in the entire big data stack. It compares Spark and Hadoop and identifies the shortcomings of Hadoop that have been overcome by Spark. The book mainly focuses on the in-depth architecture of Spark and our understanding of Spark RDDs and how RDD complements big data\u2019s immutable nature, and solves it with lazy evaluation, cacheable and type inference. It also addresses advanced topics in Spark, starting with the basics of Scala and the core Spark framework, and exploring Spark data frames, machine learning using Mllib, graph analytics using Graph X and real-time processing with Apache Kafka, AWS Kenisis, and Azure Event Hub. It then goes on to investigate Spark using PySpark and R. Focusing on the current big data stack, the book examines the interaction with current big data tools, with Spark being the core processing layer for all types of data. The book is intended for data engineers and scientists working on massive datasets and big data technologies in the cloud. In addition to industry professionals, it is helpful for aspiring data processing professionals and students working in big data processing and cloud computing environments.","language":"en","currency":"USD","id":"9811305501","title":"Big Data Processing Using Spark in Cloud","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=wzFgDwAAQBAJ&source=gbs_api","authors":["Mamta Mittal","Valentina E. Balas","Lalit Mohan Goyal","Raghvendra Kumar"]},{"pageCount":265,"thumbnail":"http:\/\/books.google.com\/books\/content?id=_WZCDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":"A Problem-Solution Approach with PySpark2","description":"Quickly find solutions to common programming problems encountered while processing big data. Content is presented in the popular problem-solution format. Look up the programming problem that you want to solve. Read the solution. Apply the solution directly in your own code. Problem solved! PySpark Recipes covers Hadoop and its shortcomings. The architecture of Spark, PySpark, and RDD are presented. You will learn to apply RDD to solve day-to-day big data problems. Python and NumPy are included and make it easy for new learners of PySpark to understand and adopt the model. What You Will Learn Understand the advanced features of PySpark2 and SparkSQL Optimize your code Program SparkSQL with Python Use Spark Streaming and Spark MLlib with Python Perform graph analysis with GraphFrames Who This Book Is For Data analysts, Python programmers, big data enthusiasts","language":"en","currency":"USD","id":"1484231414","title":"PySpark Recipes","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=_WZCDwAAQBAJ&source=gbs_api","authors":["Raju Kumar Mishra"]},{"pageCount":616,"thumbnail":"http:\/\/books.google.com\/books\/content?id=p1heDgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":29.99,"subtitle":"The Big Ideas Behind Reliable, Scalable, and Maintainable Systems","description":"Data is at the center of many challenges in system design today. Difficult issues need to be figured out, such as scalability, consistency, reliability, efficiency, and maintainability. In addition, we have an overwhelming variety of tools, including relational databases, NoSQL datastores, stream or batch processors, and message brokers. What are the right choices for your application? How do you make sense of all these buzzwords? In this practical and comprehensive guide, author Martin Kleppmann helps you navigate this diverse landscape by examining the pros and cons of various technologies for processing and storing data. Software keeps changing, but the fundamental principles remain the same. With this book, software engineers and architects will learn how to apply those ideas in practice, and how to make full use of data in modern applications. Peer under the hood of the systems you already use, and learn how to use and operate them more effectively Make informed decisions by identifying the strengths and weaknesses of different tools Navigate the trade-offs around consistency, scalability, fault tolerance, and complexity Understand the distributed systems research upon which modern databases are built Peek behind the scenes of major online services, and learn from their architectures","language":"en","currency":"USD","id":"1491903104","title":"Designing Data-Intensive Applications","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=p1heDgAAQBAJ&source=gbs_api","authors":["Martin Kleppmann"]},{"pageCount":366,"thumbnail":"http:\/\/books.google.com\/books\/content?id=5jDtDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":47.39,"subtitle":"Learning to Understand Text at Scale","description":"If you want to build an enterprise-quality application that uses natural language text but aren\u2019t sure where to begin or what tools to use, this practical guide will help get you started. Alex Thomas, principal data scientist at Wisecube, shows software engineers and data scientists how to build scalable natural language processing (NLP) applications using deep learning and the Apache Spark NLP library. Through concrete examples, practical and theoretical explanations, and hands-on exercises for using NLP on the Spark processing framework, this book teaches you everything from basic linguistics and writing systems to sentiment analysis and search engines. You\u2019ll also explore special concerns for developing text-based applications, such as performance. In four sections, you\u2019ll learn NLP basics and building blocks before diving into application and system building: Basics: Understand the fundamentals of natural language processing, NLP on Apache Stark, and deep learning Building blocks: Learn techniques for building NLP applications\u2014including tokenization, sentence segmentation, and named-entity recognition\u2014and discover how and why they work Applications: Explore the design, development, and experimentation process for building your own NLP applications Building NLP systems: Consider options for productionizing and deploying NLP models, including which human languages to support","language":"en","currency":"USD","id":"1492047716","title":"Natural Language Processing with Spark NLP","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=5jDtDwAAQBAJ&source=gbs_api","authors":["Alex Thomas"]},{"pageCount":202,"thumbnail":"http:\/\/books.google.com\/books\/content?id=JqunDAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":7.81,"subtitle":null,"description":"Hay mucha excitaci√≥n en relaci√≥n con el an√°lisis del big data, pero tambi√©n mucha confusi√≥n en decidir por d√≥nde empezar para aquellos que quieren iniciarse en la programaci√≥n en este apasionante mundo. Este libro proporciona al lector una oportunidad para empezar a programar y manejar datos a trav√©s del ecosistema Apache Spark. Spark es actualmente uno de los paquetes de c√≥digo abierto m√°s importantes en el espacio del big data y por el que importantes empresas, como IBM, SAP, Oracle o Amazon, han apostado, al tiempo que son tambi√©n grandes contribuidoras. Este libro, que puede utilizarse como texto de autoestudio o de soporte a cursos que requieran una introducci√≥n a Apache Spark, contiene una excelente visi√≥n introductoria de Apache Spark, una descripci√≥n de su ecosistema y de sus caracter√≠sticas b√°sicas e incluye ejemplos de c√≥digo para que el lector los pueda probar en su propio PC si lo desea y as√≠ tener una comprensi√≥n de primera mano de algunas de sus posibilidades.","language":"es","currency":"USD","id":"8491160493","title":"Introducci√≥n a Apache Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=JqunDAAAQBAJ&source=gbs_api","authors":["Mario Mac√≠as","Mauro G√≥mez","Ruben Tous","Jordi Torres"]},{"pageCount":277,"thumbnail":"http:\/\/books.google.com\/books\/content?id=yqhPCwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.72,"subtitle":"A Practitioner's Guide to Using Spark for Large Scale Data Analysis","description":"Big Data Analytics with Spark is a step-by-step guide for learning Spark, which is an open-source fast and general-purpose cluster computing framework for large-scale data analysis. You will learn how to use Spark for different types of big data analytics projects, including batch, interactive, graph, and stream data analysis as well as machine learning. In addition, this book will help you become a much sought-after Spark expert. Spark is one of the hottest Big Data technologies. The amount of data generated today by devices, applications and users is exploding. Therefore, there is a critical need for tools that can analyze large-scale data and unlock value from it. Spark is a powerful technology that meets that need. You can, for example, use Spark to perform low latency computations through the use of efficient caching and iterative algorithms; leverage the features of its shell for easy and interactive Data analysis; employ its fast batch processing and low latency features to process your real time data streams and so on. As a result, adoption of Spark is rapidly growing and is replacing Hadoop MapReduce as the technology of choice for big data analytics. This book provides an introduction to Spark and related big-data technologies. It covers Spark core and its add-on libraries, including Spark SQL, Spark Streaming, GraphX, and MLlib. Big Data Analytics with Spark is therefore written for busy professionals who prefer learning a new technology from a consolidated source instead of spending countless hours on the Internet trying to pick bits and pieces from different sources. The book also provides a chapter on Scala, the hottest functional programming language, and the program that underlies Spark. You\u2019ll learn the basics of functional programming in Scala, so that you can write Spark applications in it. What's more, Big Data Analytics with Spark provides an introduction to other big data technologies that are commonly used along with Spark, like Hive, Avro, Kafka and so on. So the book is self-sufficient; all the technologies that you need to know to use Spark are covered. The only thing that you are expected to know is programming in any language. There is a critical shortage of people with big data expertise, so companies are willing to pay top dollar for people with skills in areas like Spark and Scala. So reading this book and absorbing its principles will provide a boost\u2014possibly a big boost\u2014to your career.","language":"en","currency":"USD","id":"1484209648","title":"Big Data Analytics with Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=yqhPCwAAQBAJ&source=gbs_api","authors":["Mohammed Guller"]},{"pageCount":352,"thumbnail":"http:\/\/books.google.com\/books\/content?id=0gVwDQAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.72,"subtitle":null,"description":"The ultimate guide to managing, building, and deploying large-scale clusters with Apache Mesos About This Book Master the architecture of Mesos and intelligently distribute your task across clusters of machines Explore a wide range of tools and platforms that Mesos works with This real-world comprehensive and robust tutorial will help you become an expert Who This Book Is For The book aims to serve DevOps engineers and system administrators who are familiar with the basics of managing a Linux system and its tools What You Will Learn Understand the Mesos architecture Manually spin up a Mesos cluster on a distributed infrastructure Deploy a multi-node Mesos cluster using your favorite DevOps See the nuts and bolts of scheduling, service discovery, failure handling, security, monitoring, and debugging in an enterprise-grade, production cluster deployment Use Mesos to deploy big data frameworks, containerized applications, or even custom build your own applications effortlessly In Detail Apache Mesos is open source cluster management software that provides efficient resource isolations and resource sharing distributed applications or frameworks. This book will take you on a journey to enhance your knowledge from amateur to master level, showing you how to improve the efficiency, management, and development of Mesos clusters. The architecture is quite complex and this book will explore the difficulties and complexities of working with Mesos. We begin by introducing Mesos, explaining its architecture and functionality. Next, we provide a comprehensive overview of Mesos features and advanced topics such as high availability, fault tolerance, scaling, and efficiency. Furthermore, you will learn to set up multi-node Mesos clusters on private and public clouds. We will also introduce several Mesos-based scheduling and management frameworks or applications to enable the easy deployment, discovery, load balancing, and failure handling of long-running services. Next, you will find out how a Mesos cluster can be easily set up and monitored using the standard deployment and configuration management tools. This advanced guide will show you how to deploy important big data processing frameworks such as Hadoop, Spark, and Storm on Mesos and big data storage frameworks such as Cassandra, Elasticsearch, and Kafka. Style and approach This advanced guide provides a detailed step-by-step account of deploying a Mesos cluster. It will demystify the concepts behind Mesos.","language":"en","currency":"USD","id":"1785885375","title":"Mastering Mesos","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=0gVwDQAAQBAJ&source=gbs_api","authors":["Dipa Dubhashi","Akhil Das"]},{"pageCount":400,"thumbnail":"http:\/\/books.google.com\/books\/content?id=VIkNCgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.81,"subtitle":"Designing Real-World Big Data Applications","description":"Get expert guidance on architecting end-to-end data management solutions with Apache Hadoop. While many sources explain how to use various components in the Hadoop ecosystem, this practical book takes you through architectural considerations necessary to tie those components together into a complete tailored application, based on your particular use case. To reinforce those lessons, the book\u2019s second section provides detailed examples of architectures used in some of the most commonly found Hadoop applications. Whether you\u2019re designing a new Hadoop application, or planning to integrate Hadoop into your existing data infrastructure, Hadoop Application Architectures will skillfully guide you through the process. This book covers: Factors to consider when using Hadoop to store and model data Best practices for moving data in and out of the system Data processing frameworks, including MapReduce, Spark, and Hive Common Hadoop processing patterns, such as removing duplicate records and using windowing analytics Giraph, GraphX, and other tools for large graph processing on Hadoop Using workflow orchestration and scheduling tools such as Apache Oozie Near-real-time stream processing with Apache Storm, Apache Spark Streaming, and Apache Flume Architecture examples for clickstream analysis, fraud detection, and data warehousing","language":"en","currency":"USD","id":"1491900059","title":"Hadoop Application Architectures","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=VIkNCgAAQBAJ&source=gbs_api","authors":["Mark Grover","Ted Malaska","Jonathan Seidman","Gwen Shapira"]},{"pageCount":202,"thumbnail":"http:\/\/books.google.com\/books\/content?id=7tZOCwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":null,"description":"Building scalable and fault-tolerant streaming applications made easy with Spark streaming About This Book Process live data streams more efficiently with better fault recovery using Spark Streaming Implement and deploy real-time log file analysis Learn about integration with Advance Spark Libraries \u2013 GraphX, Spark SQL, and MLib. Who This Book Is For This book is intended for big data developers with basic knowledge of Scala but no knowledge of Spark. It will help you grasp the basics of developing real-time applications with Spark and understand efficient programming of core elements and applications. What You Will Learn Install and configure Spark and Spark Streaming to execute applications Explore the architecture and components of Spark and Spark Streaming to use it as a base for other libraries Process distributed log files in real-time to load data from distributed sources Apply transformations on streaming data to use its functions Integrate Apache Spark with the various advance libraries like MLib and GraphX Apply production deployment scenarios to deploy your application In Detail Using practical examples with easy-to-follow steps, this book will teach you how to build real-time applications with Spark Streaming. Starting with installing and setting the required environment, you will write and execute your first program for Spark Streaming. This will be followed by exploring the architecture and components of Spark Streaming along with an overview of libraries\/functions exposed by Spark. Next you will be taught about various client APIs for coding in Spark by using the use-case of distributed log file processing. You will then apply various functions to transform and enrich streaming data. Next you will learn how to cache and persist datasets. Moving on you will integrate Apache Spark with various other libraries\/components of Spark like Mlib, GraphX, and Spark SQL. Finally, you will learn about deploying your application and cover the different scenarios ranging from standalone mode to distributed mode using Mesos, Yarn, and private data centers or on cloud infrastructure. Style and approach A Step-by-Step approach to learn Spark Streaming in a structured manner, with detailed explanation of basic and advance features in an easy-to-follow Style. Each topic is explained sequentially and supported with real world examples and executable code snippets that appeal to the needs of readers with the wide range of experiences.","language":"en","currency":"USD","id":"1783987677","title":"Learning Real-time Processing with Spark Streaming","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=7tZOCwAAQBAJ&source=gbs_api","authors":["Sumit Gupta"]},{"pageCount":360,"thumbnail":"http:\/\/books.google.com\/books\/content?id=9JlGDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":"Distributed Computing and Event Processing using Apache Spark, Flink, Storm, and Kafka","description":"A practical guide to help you tackle different real-time data processing and analytics problems using the best tools for each scenario About This Book Learn about the various challenges in real-time data processing and use the right tools to overcome them This book covers popular tools and frameworks such as Spark, Flink, and Apache Storm to solve all your distributed processing problems A practical guide filled with examples, tips, and tricks to help you perform efficient Big Data processing in real-time Who This Book Is For If you are a Java developer who would like to be equipped with all the tools required to devise an end-to-end practical solution on real-time data streaming, then this book is for you. Basic knowledge of real-time processing would be helpful, and knowing the fundamentals of Maven, Shell, and Eclipse would be great. What You Will Learn Get an introduction to the established real-time stack Understand the key integration of all the components Get a thorough understanding of the basic building blocks for real-time solution designing Garnish the search and visualization aspects for your real-time solution Get conceptually and practically acquainted with real-time analytics Be well equipped to apply the knowledge and create your own solutions In Detail With the rise of Big Data, there is an increasing need to process large amounts of data continuously, with a shorter turnaround time. Real-time data processing involves continuous input, processing and output of data, with the condition that the time required for processing is as short as possible. This book covers the majority of the existing and evolving open source technology stack for real-time processing and analytics. You will get to know about all the real-time solution aspects, from the source to the presentation to persistence. Through this practical book, you'll be equipped with a clear understanding of how to solve challenges on your own. We'll cover topics such as how to set up components, basic executions, integrations, advanced use cases, alerts, and monitoring. You'll be exposed to the popular tools used in real-time processing today such as Apache Spark, Apache Flink, and Storm. Finally, you will put your knowledge to practical use by implementing all of the techniques in the form of a practical, real-world use case. By the end of this book, you will have a solid understanding of all the aspects of real-time data processing and analytics, and will know how to deploy the solutions in production environments in the best possible manner. Style and Approach In this practical guide to real-time analytics, each chapter begins with a basic high-level concept of the topic, followed by a practical, hands-on implementation of each concept, where you can see the working and execution of it. The book is written in a DIY style, with plenty of practical use cases, well-explained code examples, and relevant screenshots and diagrams.","language":"en","currency":"USD","id":"1787289869","title":"Practical Real-time Data Processing and Analytics","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=9JlGDwAAQBAJ&source=gbs_api","authors":["Shilpi Saxena","Saurabh Gupta"]},{"pageCount":141,"thumbnail":"http:\/\/books.google.com\/books\/content?id=TNUlDAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":43.96,"subtitle":null,"description":"The past few years have seen a major change in computing systems, as growing data volumes and stalling processor speeds require more and more applications to scale out to clusters. Today, a myriad data sources, from the Internet to business operations to scientific instruments, produce large and valuable data streams. However, the processing capabilities of single machines have not kept up with the size of data. As a result, organizations increasingly need to scale out their computations over clusters. At the same time, the speed and sophistication required of data processing have grown. In addition to simple queries, complex algorithms like machine learning and graph analysis are becoming common. And in addition to batch processing, streaming analysis of real-time data is required to let organizations take timely action. Future computing platforms will need to not only scale out traditional workloads, but support these new applications too. This book, a revised version of the 2014 ACM Dissertation Award winning dissertation, proposes an architecture for cluster computing systems that can tackle emerging data processing workloads at scale. Whereas early cluster computing systems, like MapReduce, handled batch processing, our architecture also enables streaming and interactive queries, while keeping MapReduce's scalability and fault tolerance. And whereas most deployed systems only support simple one-pass computations (e.g., SQL queries), ours also extends to the multi-pass algorithms required for complex analytics like machine learning. Finally, unlike the specialized systems proposed for some of these workloads, our architecture allows these computations to be combined, enabling rich new applications that intermix, for example, streaming and batch processing. We achieve these results through a simple extension to MapReduce that adds primitives for data sharing, called Resilient Distributed Datasets (RDDs). We show that this is enough to capture a wide range of workloads. We implement RDDs in the open source Spark system, which we evaluate using synthetic and real workloads. Spark matches or exceeds the performance of specialized systems in many domains, while offering stronger fault tolerance properties and allowing these workloads to be combined. Finally, we examine the generality of RDDs from both a theoretical modeling perspective and a systems perspective. This version of the dissertation makes corrections throughout the text and adds a new section on the evolution of Apache Spark in industry since 2014. In addition, editing, formatting, and links for the references have been added.","language":"en","currency":"USD","id":"1970001577","title":"An Architecture for Fast and General Data Processing on Large Clusters","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=TNUlDAAAQBAJ&source=gbs_api","authors":["Matei Zaharia"]},{"pageCount":298,"thumbnail":"http:\/\/books.google.com\/books\/content?id=Z1qWDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":22.39,"subtitle":"Perform data collection, processing, manipulation, and visualization with Scala","description":"Master scala's advanced techniques to solve real-world problems in data analysis and gain valuable insights from your data Key Features A beginner's guide for performing data analysis loaded with numerous rich, practical examples Access to popular Scala libraries such as Breeze, Saddle for efficient data manipulation and exploratory analysis Develop applications in Scala for real-time analysis and machine learning in Apache Spark Book Description Efficient business decisions with an accurate sense of business data helps in delivering better performance across products and services. This book helps you to leverage the popular Scala libraries and tools for performing core data analysis tasks with ease. The book begins with a quick overview of the building blocks of a standard data analysis process. You will learn to perform basic tasks like Extraction, Staging, Validation, Cleaning, and Shaping of datasets. You will later deep dive into the data exploration and visualization areas of the data analysis life cycle. You will make use of popular Scala libraries like Saddle, Breeze, Vegas, and PredictionIO for processing your datasets. You will learn statistical methods for deriving meaningful insights from data. You will also learn to create applications for Apache Spark 2.x on complex data analysis, in real-time. You will discover traditional machine learning techniques for doing data analysis. Furthermore, you will also be introduced to neural networks and deep learning from a data analysis standpoint. By the end of this book, you will be capable of handling large sets of structured and unstructured data, perform exploratory analysis, and building efficient Scala applications for discovering and delivering insights What you will learn Techniques to determine the validity and confidence level of data Apply quartiles and n-tiles to datasets to see how data is distributed into many buckets Create data pipelines that combine multiple data lifecycle steps Use built-in features to gain a deeper understanding of the data Apply Lasso regression analysis method to your data Compare Apache Spark API with traditional Apache Spark data analysis Who this book is for If you are a data scientist or a data analyst who wants to learn how to perform data analysis using Scala, this book is for you. All you need is knowledge of the basic fundamentals of Scala programming.","language":"en","currency":"USD","id":"1789344263","title":"Hands-On Data Analysis with Scala","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=Z1qWDwAAQBAJ&source=gbs_api","authors":["Rajesh Gupta"]},{"pageCount":101,"thumbnail":"http:\/\/books.google.com\/books\/content?id=dGu-BQAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":0,"subtitle":null,"description":"Big data is currently one of the most critical emerging technologies. Organizations around the world are looking to exploit the explosive growth of data to unlock previously hidden insights in the hope of creating new revenue streams, gaining operational efficiencies, and obtaining greater understanding of customer needs. It is important to think of big data and analytics together. Big data is the term used to describe the recent explosion of different types of data from disparate sources. Analytics is about examining data to derive interesting and relevant trends and patterns, which can be used to inform decisions, optimize processes, and even drive new business models. With today's deluge of data comes the problems of processing that data, obtaining the correct skills to manage and analyze that data, and establishing rules to govern the data's use and distribution. The big data technology stack is ever growing and sometimes confusing, even more so when we add the complexities of setting up big data environments with large up-front investments. Cloud computing seems to be a perfect vehicle for hosting big data workloads. However, working on big data in the cloud brings its own challenge of reconciling two contradictory design principles. Cloud computing is based on the concepts of consolidation and resource pooling, but big data systems (such as Hadoop) are built on the shared nothing principle, where each node is independent and self-sufficient. A solution architecture that can allow these mutually exclusive principles to coexist is required to truly exploit the elasticity and ease-of-use of cloud computing for big data environments. This IBM¬Æ RedpaperTM publication is aimed at chief architects, line-of-business executives, and CIOs to provide an understanding of the cloud-related challenges they face and give prescriptive guidance for how to realize the benefits of big data solutions quickly and cost-effectively.","language":"en","currency":"USD","id":"0738453994","title":"Building Big Data and Analytics Solutions in the Cloud","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=dGu-BQAAQBAJ&source=gbs_api","authors":["Wei-Dong Zhu","Manav Gupta","Ven Kumar","Sujatha Perepa","Arvind Sathi","Craig Statchuk","IBM Redbooks"]},{"pageCount":338,"thumbnail":"http:\/\/books.google.com\/books\/content?id=woBcDgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":null,"description":"A practical guide to obtaining, transforming, exploring, and analyzing data using Python, MongoDB, and Apache Spark About This Book Learn to use various data analysis tools and algorithms to classify, cluster, visualize, simulate, and forecast your data Apply Machine Learning algorithms to different kinds of data such as social networks, time series, and images A hands-on guide to understanding the nature of data and how to turn it into insight Who This Book Is For This book is for developers who want to implement data analysis and data-driven algorithms in a practical way. It is also suitable for those without a background in data analysis or data processing. Basic knowledge of Python programming, statistics, and linear algebra is assumed. What You Will Learn Acquire, format, and visualize your data Build an image-similarity search engine Generate meaningful visualizations anyone can understand Get started with analyzing social network graphs Find out how to implement sentiment text analysis Install data analysis tools such as Pandas, MongoDB, and Apache Spark Get to grips with Apache Spark Implement machine learning algorithms such as classification or forecasting In Detail Beyond buzzwords like Big Data or Data Science, there are a great opportunities to innovate in many businesses using data analysis to get data-driven products. Data analysis involves asking many questions about data in order to discover insights and generate value for a product or a service. This book explains the basic data algorithms without the theoretical jargon, and you'll get hands-on turning data into insights using machine learning techniques. We will perform data-driven innovation processing for several types of data such as text, Images, social network graphs, documents, and time series, showing you how to implement large data processing with MongoDB and Apache Spark. Style and approach This is a hands-on guide to data analysis and data processing. The concrete examples are explained with simple code and accessible data.","language":"en","currency":"USD","id":"1785286668","title":"Practical Data Analysis","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=woBcDgAAQBAJ&source=gbs_api","authors":["Hector Cuesta","Dr. Sampath Kumar"]},{"pageCount":722,"thumbnail":"http:\/\/books.google.com\/books\/content?id=xVU2AAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":43.11,"subtitle":"Recipes for Object-Oriented and Functional Programming","description":"Save time and trouble when using Scala to build object-oriented, functional, and concurrent applications. With more than 250 ready-to-use recipes and 700 code examples, this comprehensive cookbook covers the most common problems you\u2019ll encounter when using the Scala language, libraries, and tools. It\u2019s ideal not only for experienced Scala developers, but also for programmers learning to use this JVM language. Author Alvin Alexander (creator of DevDaily.com) provides solutions based on his experience using Scala for highly scalable, component-based applications that support concurrency and distribution. Packed with real-world scenarios, this book provides recipes for: Strings, numeric types, and control structures Classes, methods, objects, traits, and packaging Functional programming in a variety of situations Collections covering Scala's wealth of classes and methods Concurrency, using the Akka Actors library Using the Scala REPL and the Simple Build Tool (SBT) Web services on both the client and server sides Interacting with SQL and NoSQL databases Best practices in Scala development","language":"en","currency":"USD","id":"1449340326","title":"Scala Cookbook","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=xVU2AAAAQBAJ&source=gbs_api","authors":["Alvin Alexander"]},{"pageCount":1265,"thumbnail":"http:\/\/books.google.com\/books\/content?id=CVQoDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":55.99,"subtitle":null,"description":"Leverage the power of Scala and master the art of building, improving, and validating scalable machine learning and AI applications using Scala's most advanced and finest features About This Book Build functional, type-safe routines to interact with relational and NoSQL databases with the help of the tutorials and examples provided Leverage your expertise in Scala programming to create and customize your own scalable machine learning algorithms Experiment with different techniques; evaluate their benefits and limitations using real-world financial applications Get to know the best practices to incorporate new Big Data machine learning in your data-driven enterprise and gain future scalability and maintainability Who This Book Is For This Learning Path is for engineers and scientists who are familiar with Scala and want to learn how to create, validate, and apply machine learning algorithms. It will also benefit software developers with a background in Scala programming who want to apply machine learning. What You Will Learn Create Scala web applications that couple with JavaScript libraries such as D3 to create compelling interactive visualizations Deploy scalable parallel applications using Apache Spark, loading data from HDFS or Hive Solve big data problems with Scala parallel collections, Akka actors, and Apache Spark clusters Apply key learning strategies to perform technical analysis of financial markets Understand the principles of supervised and unsupervised learning in machine learning Work with unstructured data and serialize it using Kryo, Protobuf, Avro, and AvroParquet Construct reliable and robust data pipelines and manage data in a data-driven enterprise Implement scalable model monitoring and alerts with Scala In Detail This Learning Path aims to put the entire world of machine learning with Scala in front of you. Scala for Data Science, the first module in this course, is a tutorial guide that provides tutorials on some of the most common Scala libraries for data science, allowing you to quickly get up to speed building data science and data engineering solutions. The second course, Scala for Machine Learning guides you through the process of building AI applications with diagrams, formal mathematical notation, source code snippets, and useful tips. A review of the Akka framework and Apache Spark clusters concludes the tutorial. The next module, Mastering Scala Machine Learning, is the final step in this course. It will take your knowledge to next level and help you use the knowledge to build advanced applications such as social media mining, intelligent news portals, and more. After a quick refresher on functional programming concepts using REPL, you will see some practical examples of setting up the development environment and tinkering with data. We will then explore working with Spark and MLlib using k-means and decision trees. By the end of this course, you will be a master at Scala machine learning and have enough expertise to be able to build complex machine learning projects using Scala. This Learning Path combines some of the best that Packt has to offer in one complete, curated package. It includes content from the following Packt products: Scala for Data Science, Pascal Bugnion Scala for Machine Learning, Patrick Nicolas Mastering Scala Machine Learning, Alex Kozlov Style and approach A tutorial with complete examples, this course will give you the tools to start building useful data engineering and data science solutions straightaway. This course provides practical examples from the field on how to correctly tackle data analysis problems, particularly for modern Big Data datasets.","language":"en","currency":"USD","id":"178712455X","title":"Scala:Applied Machine Learning","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=CVQoDwAAQBAJ&source=gbs_api","authors":["Pascal Bugnion","Patrick R. Nicolas","Alex Kozlov"]},{"pageCount":104,"thumbnail":"http:\/\/books.google.com\/books\/content?id=DRqkBwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":12.75,"subtitle":null,"description":"If you\u2019re a business team leader, CIO, business analyst, or developer interested in how Apache Hadoop and Apache HBase-related technologies can address problems involving large-scale data in cost-effective ways, this book is for you. Using real-world stories and situations, authors Ted Dunning and Ellen Friedman show Hadoop newcomers and seasoned users alike how NoSQL databases and Hadoop can solve a variety of business and research issues. You\u2019ll learn about early decisions and pre-planning that can make the process easier and more productive. If you\u2019re already using these technologies, you\u2019ll discover ways to gain the full range of benefits possible with Hadoop. While you don\u2019t need a deep technical background to get started, this book does provide expert guidance to help managers, architects, and practitioners succeed with their Hadoop projects. Examine a day in the life of big data: India\u2019s ambitious Aadhaar project Review tools in the Hadoop ecosystem such as Apache\u2019s Spark, Storm, and Drill to learn how they can help you Pick up a collection of technical and strategic tips that have helped others succeed with Hadoop Learn from several prototypical Hadoop use cases, based on how organizations have actually applied the technology Explore real-world stories that reveal how MapR customers combine use cases when putting Hadoop and NoSQL to work, including in production","language":"en","currency":"USD","id":"1491928913","title":"Real-World Hadoop","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=DRqkBwAAQBAJ&source=gbs_api","authors":["Ted Dunning","Ellen Friedman"]},{"pageCount":722,"thumbnail":"http:\/\/books.google.com\/books\/content?id=xVU2AAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":43.11,"subtitle":"Recipes for Object-Oriented and Functional Programming","description":"Save time and trouble when using Scala to build object-oriented, functional, and concurrent applications. With more than 250 ready-to-use recipes and 700 code examples, this comprehensive cookbook covers the most common problems you\u2019ll encounter when using the Scala language, libraries, and tools. It\u2019s ideal not only for experienced Scala developers, but also for programmers learning to use this JVM language. Author Alvin Alexander (creator of DevDaily.com) provides solutions based on his experience using Scala for highly scalable, component-based applications that support concurrency and distribution. Packed with real-world scenarios, this book provides recipes for: Strings, numeric types, and control structures Classes, methods, objects, traits, and packaging Functional programming in a variety of situations Collections covering Scala's wealth of classes and methods Concurrency, using the Akka Actors library Using the Scala REPL and the Simple Build Tool (SBT) Web services on both the client and server sides Interacting with SQL and NoSQL databases Best practices in Scala development","language":"en","currency":"USD","id":"1449340326","title":"Scala Cookbook","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=xVU2AAAAQBAJ&source=gbs_api","authors":["Alvin Alexander"]},{"pageCount":416,"thumbnail":"http:\/\/books.google.com\/books\/content?id=9V0MEAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":36,"subtitle":"Specialty (DAS-C01) Exam","description":"Move your career forward with AWS certification! Prepare for the AWS Certified Data Analytics Specialty Exam with this thorough study guide This comprehensive study guide will help assess your technical skills and prepare for the updated AWS Certified Data Analytics exam. Earning this AWS certification will confirm your expertise in designing and implementing AWS services to derive value from data. The AWS Certified Data Analytics Study Guide: Specialty (DAS-C01) Exam is designed for business analysts and IT professionals who perform complex Big Data analyses. This AWS Specialty Exam guide gets you ready for certification testing with expert content, real-world knowledge, key exam concepts, and topic reviews. Gain confidence by studying the subject areas and working through the practice questions. Big data concepts covered in the guide include: Collection Storage Processing Analysis Visualization Data security AWS certifications allow professionals to demonstrate skills related to leading Amazon Web Services technology. The AWS Certified Data Analytics Specialty (DAS-C01) Exam specifically evaluates your ability to design and maintain Big Data, leverage tools to automate data analysis, and implement AWS Big Data services according to architectural best practices. An exam study guide can help you feel more prepared about taking an AWS certification test and advancing your professional career. In addition to the guide\u2019s content, you\u2019ll have access to an online learning environment and test bank that offers practice exams, a glossary, and electronic flashcards.","language":"en","currency":"USD","id":"1119649455","title":"AWS Certified Data Analytics Study Guide","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=9V0MEAAAQBAJ&source=gbs_api","authors":["Asif Abbasi"]},{"pageCount":786,"thumbnail":"http:\/\/books.google.com\/books\/content?id=7eZDDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":41.07,"subtitle":"Explore the concepts of functional programming, data streaming, and machine learning","description":"Harness the power of Scala to program Spark and analyze tonnes of data in the blink of an eye! About This Book Learn Scala's sophisticated type system that combines Functional Programming and object-oriented concepts Work on a wide array of applications, from simple batch jobs to stream processing and machine learning Explore the most common as well as some complex use-cases to perform large-scale data analysis with Spark Who This Book Is For Anyone who wishes to learn how to perform data analysis by harnessing the power of Spark will find this book extremely useful. No knowledge of Spark or Scala is assumed, although prior programming experience (especially with other JVM languages) will be useful to pick up concepts quicker. What You Will Learn Understand object-oriented & functional programming concepts of Scala In-depth understanding of Scala collection APIs Work with RDD and DataFrame to learn Spark's core abstractions Analysing structured and unstructured data using SparkSQL and GraphX Scalable and fault-tolerant streaming application development using Spark structured streaming Learn machine-learning best practices for classification, regression, dimensionality reduction, and recommendation system to build predictive models with widely used algorithms in Spark MLlib & ML Build clustering models to cluster a vast amount of data Understand tuning, debugging, and monitoring Spark applications Deploy Spark applications on real clusters in Standalone, Mesos, and YARN In Detail Scala has been observing wide adoption over the past few years, especially in the field of data science and analytics. Spark, built on Scala, has gained a lot of recognition and is being used widely in productions. Thus, if you want to leverage the power of Scala and Spark to make sense of big data, this book is for you. The first part introduces you to Scala, helping you understand the object-oriented and functional programming concepts needed for Spark application development. It then moves on to Spark to cover the basic abstractions using RDD and DataFrame. This will help you develop scalable and fault-tolerant streaming applications by analyzing structured and unstructured data using SparkSQL, GraphX, and Spark structured streaming. Finally, the book moves on to some advanced topics, such as monitoring, configuration, debugging, testing, and deployment. You will also learn how to develop Spark applications using SparkR and PySpark APIs, interactive data analytics using Zeppelin, and in-memory data processing with Alluxio. By the end of this book, you will have a thorough understanding of Spark, and you will be able to perform full-stack data analytics with a feel that no amount of data is too big. Style and approach Filled with practical examples and use cases, this book will hot only help you get up and running with Spark, but will also take you farther down the road to becoming a data scientist.","language":"en","currency":"USD","id":"1783550503","title":"Scala and Spark for Big Data Analytics","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=7eZDDwAAQBAJ&source=gbs_api","authors":["Md. Rezaul Karim","Sridhar Alla"]},{"pageCount":1265,"thumbnail":"http:\/\/books.google.com\/books\/content?id=CVQoDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":55.99,"subtitle":null,"description":"Leverage the power of Scala and master the art of building, improving, and validating scalable machine learning and AI applications using Scala's most advanced and finest features About This Book Build functional, type-safe routines to interact with relational and NoSQL databases with the help of the tutorials and examples provided Leverage your expertise in Scala programming to create and customize your own scalable machine learning algorithms Experiment with different techniques; evaluate their benefits and limitations using real-world financial applications Get to know the best practices to incorporate new Big Data machine learning in your data-driven enterprise and gain future scalability and maintainability Who This Book Is For This Learning Path is for engineers and scientists who are familiar with Scala and want to learn how to create, validate, and apply machine learning algorithms. It will also benefit software developers with a background in Scala programming who want to apply machine learning. What You Will Learn Create Scala web applications that couple with JavaScript libraries such as D3 to create compelling interactive visualizations Deploy scalable parallel applications using Apache Spark, loading data from HDFS or Hive Solve big data problems with Scala parallel collections, Akka actors, and Apache Spark clusters Apply key learning strategies to perform technical analysis of financial markets Understand the principles of supervised and unsupervised learning in machine learning Work with unstructured data and serialize it using Kryo, Protobuf, Avro, and AvroParquet Construct reliable and robust data pipelines and manage data in a data-driven enterprise Implement scalable model monitoring and alerts with Scala In Detail This Learning Path aims to put the entire world of machine learning with Scala in front of you. Scala for Data Science, the first module in this course, is a tutorial guide that provides tutorials on some of the most common Scala libraries for data science, allowing you to quickly get up to speed building data science and data engineering solutions. The second course, Scala for Machine Learning guides you through the process of building AI applications with diagrams, formal mathematical notation, source code snippets, and useful tips. A review of the Akka framework and Apache Spark clusters concludes the tutorial. The next module, Mastering Scala Machine Learning, is the final step in this course. It will take your knowledge to next level and help you use the knowledge to build advanced applications such as social media mining, intelligent news portals, and more. After a quick refresher on functional programming concepts using REPL, you will see some practical examples of setting up the development environment and tinkering with data. We will then explore working with Spark and MLlib using k-means and decision trees. By the end of this course, you will be a master at Scala machine learning and have enough expertise to be able to build complex machine learning projects using Scala. This Learning Path combines some of the best that Packt has to offer in one complete, curated package. It includes content from the following Packt products: Scala for Data Science, Pascal Bugnion Scala for Machine Learning, Patrick Nicolas Mastering Scala Machine Learning, Alex Kozlov Style and approach A tutorial with complete examples, this course will give you the tools to start building useful data engineering and data science solutions straightaway. This course provides practical examples from the field on how to correctly tackle data analysis problems, particularly for modern Big Data datasets.","language":"en","currency":"USD","id":"178712455X","title":"Scala:Applied Machine Learning","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=CVQoDwAAQBAJ&source=gbs_api","authors":["Pascal Bugnion","Patrick R. Nicolas","Alex Kozlov"]},{"pageCount":477,"thumbnail":"http:\/\/books.google.com\/books\/content?id=LAuwDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":69.42,"subtitle":"Third International Conference, ICAICR 2019, Shimla, India, June 15\u201316, 2019, Revised Selected Papers","description":"This two-volume set (CCIS 1075 and CCIS 1076) constitutes the refereed proceedings of the Third International Conference on Advanced Informatics for Computing Research, ICAICR 2019, held in Shimla, India, in June 2019. The 78 revised full papers presented were carefully reviewed and selected from 382 submissions. The papers are organized in topical sections on computing methodologies; hardware; information systems; networks; software and its engineering.","language":"en","currency":"USD","id":"9811501084","title":"Advanced Informatics for Computing Research","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=LAuwDwAAQBAJ&source=gbs_api","authors":["Ashish Kumar Luhach","Dharm Singh Jat","Kamarul Bin Ghazali Hawari","Xiao-Zhi Gao","Pawan Lingras"]},{"pageCount":348,"thumbnail":"http:\/\/books.google.com\/books\/content?id=K-l1DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":"An expert guide to improving database scalability and availability without compromising performance, 3rd Edition","description":"Build, manage, and configure high-performing, reliable NoSQL database for your applications with Cassandra Key Features Write programs more efficiently using Cassandra's features with the help of examples Configure Cassandra and fine-tune its parameters depending on your needs Integrate Cassandra database with Apache Spark and build strong data analytics pipeline Book Description With ever-increasing rates of data creation, the demand for storing data fast and reliably becomes a need. Apache Cassandra is the perfect choice for building fault-tolerant and scalable databases. Mastering Apache Cassandra 3.x teaches you how to build and architect your clusters, configure and work with your nodes, and program in a high-throughput environment, helping you understand the power of Cassandra as per the new features. Once you\u2019ve covered a brief recap of the basics, you\u2019ll move on to deploying and monitoring a production setup and optimizing and integrating it with other software. You\u2019ll work with the advanced features of CQL and the new storage engine in order to understand how they function on the server-side. You\u2019ll explore the integration and interaction of Cassandra components, followed by discovering features such as token allocation algorithm, CQL3, vnodes, lightweight transactions, and data modelling in detail. Last but not least you will get to grips with Apache Spark. By the end of this book, you\u2019ll be able to analyse big data, and build and manage high-performance databases for your application. What you will learn Write programs more efficiently using Cassandra's features more efficiently Exploit the given infrastructure, improve performance, and tweak the Java Virtual Machine (JVM) Use CQL3 in your application in order to simplify working with Cassandra Configure Cassandra and fine-tune its parameters depending on your needs Set up a cluster and learn how to scale it Monitor a Cassandra cluster in different ways Use Apache Spark and other big data processing tools Who this book is for Mastering Apache Cassandra 3.x is for you if you are a big data administrator, database administrator, architect, or developer who wants to build a high-performing, scalable, and fault-tolerant database. Prior knowledge of core concepts of databases is required.","language":"en","currency":"USD","id":"1789132800","title":"Mastering Apache Cassandra 3.x","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=K-l1DwAAQBAJ&source=gbs_api","authors":["Aaron Ploetz","Tejaswi Malepati","Nishant Neeraj"]},{"pageCount":220,"thumbnail":"http:\/\/books.google.com\/books\/content?id=Yuh1DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":14.59,"subtitle":"Learn about big data processing and analytics","description":"A fast paced guide that will help you learn about Apache Hadoop 3 and its ecosystem Key Features Set up, configure and get started with Hadoop to get useful insights from large data sets Work with the different components of Hadoop such as MapReduce, HDFS and YARN Learn about the new features introduced in Hadoop 3 Book Description Apache Hadoop is a widely used distributed data platform. It enables large datasets to be efficiently processed instead of using one large computer to store and process the data. This book will get you started with the Hadoop ecosystem, and introduce you to the main technical topics, including MapReduce, YARN, and HDFS. The book begins with an overview of big data and Apache Hadoop. Then, you will set up a pseudo Hadoop development environment and a multi-node enterprise Hadoop cluster. You will see how the parallel programming paradigm, such as MapReduce, can solve many complex data processing problems. The book also covers the important aspects of the big data software development lifecycle, including quality assurance and control, performance, administration, and monitoring. You will then learn about the Hadoop ecosystem, and tools such as Kafka, Sqoop, Flume, Pig, Hive, and HBase. Finally, you will look at advanced topics, including real time streaming using Apache Storm, and data analytics using Apache Spark. By the end of the book, you will be well versed with different configurations of the Hadoop 3 cluster. What you will learn Store and analyze data at scale using HDFS, MapReduce and YARN Install and configure Hadoop 3 in different modes Use Yarn effectively to run different applications on Hadoop based platform Understand and monitor how Hadoop cluster is managed Consume streaming data using Storm, and then analyze it using Spark Explore Apache Hadoop ecosystem components, such as Flume, Sqoop, HBase, Hive, and Kafka Who this book is for Aspiring Big Data professionals who want to learn the essentials of Hadoop 3 will find this book to be useful. Existing Hadoop users who want to get up to speed with the new features introduced in Hadoop 3 will also benefit from this book. Having knowledge of Java programming will be an added advantage.","language":"en","currency":"USD","id":"1788994345","title":"Apache Hadoop 3 Quick Start Guide","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=Yuh1DwAAQBAJ&source=gbs_api","authors":["Hrishikesh Vijay Karambelkar"]},{"pageCount":280,"thumbnail":"http:\/\/books.google.com\/books\/content?id=RDl4BgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":24.99,"subtitle":"Designing Fine-Grained Systems","description":"Distributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, this book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures. Microservice technologies are moving quickly. Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You\u2019ll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain. Discover how microservices allow you to align your system design with your organization\u2019s goals Learn options for integrating a service with the rest of your system Take an incremental approach when splitting monolithic codebases Deploy individual microservices through continuous integration Examine the complexities of testing and monitoring distributed services Manage security with user-to-service and service-to-service models Understand the challenges of scaling microservice architectures","language":"en","currency":"USD","id":"1491950315","title":"Building Microservices","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=RDl4BgAAQBAJ&source=gbs_api","authors":["Sam Newman"]},{"pageCount":146,"thumbnail":"http:\/\/books.google.com\/books\/content?id=MpZGDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":17.19,"subtitle":null,"description":"Over 50 recipes on the core features of Apache Mesos and running big data frameworks in Mesos About This Book Learn to install and configure Mesos to suit the needs of your organization Follow step-by-step instructions to deploy application frameworks on top of Mesos, saving you many hours of research and trial and error Use this practical guide packed with powerful recipes to implement Mesos and easily integrate it with other application frameworks Who This Book Is For This book is for system administrators, engineers, and big data programmers. Basic experience with big data technologies such as Hadoop or Spark would be useful but is not essential. A working knowledge of Apache Mesos is expected. What You Will Learn Set up Mesos on different operating systems Use the Marathon and Chronos frameworks to manage multiple applications Work with Mesos and Docker Integrate Mesos with Spark and other big data frameworks Use networking features in Mesos for effective communication between containers Configure Mesos for high availability using Zookeeper Secure your Mesos clusters with SASL and Authorization ACLs Solve everyday problems and discover the best practices In Detail Apache Mesos is open source cluster sharing and management software. Deploying and managing scalable applications in large-scale clustered environments can be difficult, but Apache Mesos makes it easier with efficient resource isolation and sharing across application frameworks. The goal of this book is to guide you through the practical implementation of the Mesos core along with a number of Mesos supported frameworks. You will begin by installing Mesos and then learn how to configure clusters and maintain them. You will also see how to deploy a cluster in a production environment with high availability using Zookeeper. Next, you will get to grips with using Mesos, Marathon, and Docker to build and deploy a PaaS. You will see how to schedule jobs with Chronos. We'll demonstrate how to integrate Mesos with big data frameworks such as Spark, Hadoop, and Storm. Practical solutions backed with clear examples will also show you how to deploy elastic big data jobs. You will find out how to deploy a scalable continuous integration and delivery system on Mesos with Jenkins. Finally, you will configure and deploy a highly scalable distributed search engine with ElasticSearch. Throughout the course of this book, you will get to know tips and tricks along with best practices to follow when working with Mesos. Style and approach This step-by-step guide is packed with powerful recipes on using Apache Mesos and shows its integration with containers and big data frameworks.","language":"en","currency":"USD","id":"1785880934","title":"Apache Mesos Cookbook","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=MpZGDwAAQBAJ&source=gbs_api","authors":["David Blomquist","Tomasz Janiszewski"]},{"pageCount":420,"thumbnail":"http:\/\/books.google.com\/books\/content?id=F-dDDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":null,"description":"This book covers the fundamentals of machine learning with Python in a concise and dynamic manner. It covers data mining and large-scale machine learning using Apache Spark. About This Book Take your first steps in the world of data science by understanding the tools and techniques of data analysis Train efficient Machine Learning models in Python using the supervised and unsupervised learning methods Learn how to use Apache Spark for processing Big Data efficiently Who This Book Is For If you are a budding data scientist or a data analyst who wants to analyze and gain actionable insights from data using Python, this book is for you. Programmers with some experience in Python who want to enter the lucrative world of Data Science will also find this book to be very useful, but you don't need to be an expert Python coder or mathematician to get the most from this book. What You Will Learn Learn how to clean your data and ready it for analysis Implement the popular clustering and regression methods in Python Train efficient machine learning models using decision trees and random forests Visualize the results of your analysis using Python's Matplotlib library Use Apache Spark's MLlib package to perform machine learning on large datasets In Detail Join Frank Kane, who worked on Amazon and IMDb's machine learning algorithms, as he guides you on your first steps into the world of data science. Hands-On Data Science and Python Machine Learning gives you the tools that you need to understand and explore the core topics in the field, and the confidence and practice to build and analyze your own machine learning models. With the help of interesting and easy-to-follow practical examples, Frank Kane explains potentially complex topics such as Bayesian methods and K-means clustering in a way that anybody can understand them. Based on Frank's successful data science course, Hands-On Data Science and Python Machine Learning empowers you to conduct data analysis and perform efficient machine learning using Python. Let Frank help you unearth the value in your data using the various data mining and data analysis techniques available in Python, and to develop efficient predictive models to predict future results. You will also learn how to perform large-scale machine learning on Big Data using Apache Spark. The book covers preparing your data for analysis, training machine learning models, and visualizing the final data analysis. Style and approach This comprehensive book is a perfect blend of theory and hands-on code examples in Python which can be used for your reference at any time.","language":"en","currency":"USD","id":"1787280225","title":"Hands-On Data Science and Python Machine Learning","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=F-dDDwAAQBAJ&source=gbs_api","authors":["Frank Kane"]},{"pageCount":462,"thumbnail":"http:\/\/books.google.com\/books\/content?id=jRCuDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.24,"subtitle":"Synthesizing Actionable Insights from Data","description":"Gain insight into essential data science skills in a holistic manner using data engineering and associated scalable computational methods. This book covers the most popular Python 3 frameworks for both local and distributed (in premise and cloud based) processing. Along the way, you will be introduced to many popular open-source frameworks, like, SciPy, scikitlearn, Numba, Apache Spark, etc. The book is structured around examples, so you will grasp core concepts via case studies and Python 3 code. As data science projects gets continuously larger and more complex, software engineering knowledge and experience is crucial to produce evolvable solutions. You'll see how to create maintainable software for data science and how to document data engineering practices. This book is a good starting point for people who want to gain practical skills to perform data science. All the code will be available in the form of IPython notebooks and Python 3 programs, which allow you to reproduce all analyses from the book and customize them for your own purpose. You'll also benefit from advanced topics like Machine Learning, Recommender Systems, and Security in Data Science. Practical Data Science with Python will empower you analyze data, formulate proper questions, and produce actionable insights, three core stages in most data science endeavors. What You'll Learn Play the role of a data scientist when completing increasingly challenging exercises using Python 3 Work work with proven data science techniques\/technologies Review scalable software engineering practices to ramp up data analysis abilities in the realm of Big Data Apply theory of probability, statistical inference, and algebra to understand the data science practices Who This Book Is For Anyone who would like to embark into the realm of data science using Python 3.","language":"en","currency":"USD","id":"1484248597","title":"Practical Data Science with Python 3","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=jRCuDwAAQBAJ&source=gbs_api","authors":["Ervin Varga"]},{"pageCount":384,"thumbnail":"http:\/\/books.google.com\/books\/content?id=uN_JDAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":null,"description":"Over 60 practical recipes on data exploration and analysis About This Book Clean dirty data, extract accurate information, and explore the relationships between variables Forecast the output of an electric plant and the water flow of American rivers using pandas, NumPy, Statsmodels, and scikit-learn Find and extract the most important features from your dataset using the most efficient Python libraries Who This Book Is For If you are a beginner or intermediate-level professional who is looking to solve your day-to-day, analytical problems with Python, this book is for you. Even with no prior programming and data analytics experience, you will be able to finish each recipe and learn while doing so. What You Will Learn Read, clean, transform, and store your data usng Pandas and OpenRefine Understand your data and explore the relationships between variables using Pandas and D3.js Explore a variety of techniques to classify and cluster outbound marketing campaign calls data of a bank using Pandas, mlpy, NumPy, and Statsmodels Reduce the dimensionality of your dataset and extract the most important features with pandas, NumPy, and mlpy Predict the output of a power plant with regression models and forecast water flow of American rivers with time series methods using pandas, NumPy, Statsmodels, and scikit-learn Explore social interactions and identify fraudulent activities with graph theory concepts using NetworkX and Gephi Scrape Internet web pages using urlib and BeautifulSoup and get to know natural language processing techniques to classify movies ratings using NLTK Study simulation techniques in an example of a gas station with agent-based modeling In Detail Data analysis is the process of systematically applying statistical and logical techniques to describe and illustrate, condense and recap, and evaluate data. Its importance has been most visible in the sector of information and communication technologies. It is an employee asset in almost all economy sectors. This book provides a rich set of independent recipes that dive into the world of data analytics and modeling using a variety of approaches, tools, and algorithms. You will learn the basics of data handling and modeling, and will build your skills gradually toward more advanced topics such as simulations, raw text processing, social interactions analysis, and more. First, you will learn some easy-to-follow practical techniques on how to read, write, clean, reformat, explore, and understand your data\u2014arguably the most time-consuming (and the most important) tasks for any data scientist. In the second section, different independent recipes delve into intermediate topics such as classification, clustering, predicting, and more. With the help of these easy-to-follow recipes, you will also learn techniques that can easily be expanded to solve other real-life problems such as building recommendation engines or predictive models. In the third section, you will explore more advanced topics: from the field of graph theory through natural language processing, discrete choice modeling to simulations. You will also get to expand your knowledge on identifying fraud origin with the help of a graph, scrape Internet websites, and classify movies based on their reviews. By the end of this book, you will be able to efficiently use the vast array of tools that the Python environment has to offer. Style and approach This hands-on recipe guide is divided into three sections that tackle and overcome real-world data modeling problems faced by data analysts\/scientist in their everyday work. Each independent recipe is written in an easy-to-follow and step-by-step fashion.","language":"en","currency":"USD","id":"1783558512","title":"Practical Data Analysis Cookbook","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=uN_JDAAAQBAJ&source=gbs_api","authors":["Tomasz Drabas"]},{"pageCount":260,"thumbnail":"http:\/\/books.google.com\/books\/content?id=XDTnDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":"Data Virtualization, Data Lake, and AI Platform","description":"Use this guide to one of SQL Server 2019\u2019s most impactful features\u2014Big Data Clusters. You will learn about data virtualization and data lakes for this complete artificial intelligence (AI) and machine learning (ML) platform within the SQL Server database engine. You will know how to use Big Data Clusters to combine large volumes of streaming data for analysis along with data stored in a traditional database. For example, you can stream large volumes of data from Apache Spark in real time while executing Transact-SQL queries to bring in relevant additional data from your corporate, SQL Server database. Filled with clear examples and use cases, this book provides everything necessary to get started working with Big Data Clusters in SQL Server 2019. You will learn about the architectural foundations that are made up from Kubernetes, Spark, HDFS, and SQL Server on Linux. You then are shown how to configure and deploy Big Data Clusters in on-premises environments or in the cloud. Next, you are taught about querying. You will learn to write queries in Transact-SQL\u2014taking advantage of skills you have honed for years\u2014and with those queries you will be able to examine and analyze data from a wide variety of sources such as Apache Spark. Through the theoretical foundation provided in this book and easy-to-follow example scripts and notebooks, you will be ready to use and unveil the full potential of SQL Server 2019: combining different types of data spread across widely disparate sources into a single view that is useful for business intelligence and machine learning analysis. What You Will Learn Install, manage, and troubleshoot Big Data Clusters in cloud or on-premise environments Analyze large volumes of data directly from SQL Server and\/or Apache Spark Manage data stored in HDFS from SQL Server as if it were relational data Implement advanced analytics solutions through machine learning and AI Expose different data sources as a single logical source using data virtualization Who This Book Is For Data engineers, data scientists, data architects, and database administrators who want to employ data virtualization and big data analytics in their environments","language":"en","currency":"USD","id":"1484259858","title":"SQL Server Big Data Clusters","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=XDTnDwAAQBAJ&source=gbs_api","authors":["Benjamin Weissman","Enrico van de Laar"]},{"pageCount":164,"thumbnail":"http:\/\/books.google.com\/books\/content?id=xAQcDAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":17.19,"subtitle":null,"description":"Learn to use Scala to build a recommendation engine from scratch and empower your website users About This Book Learn the basics of a recommendation engine and its application in e-commerce Discover the tools and machine learning methods required to build a recommendation engine Explore different kinds of recommendation engines using Scala libraries such as MLib and Spark Who This Book Is For This book is written for those who want to learn the different tools in the Scala ecosystem to build a recommendation engine. No prior knowledge of Scala or recommendation engines is assumed. What You Will Learn Discover the tools in the Scala ecosystem Understand the challenges faced in e-commerce systems and learn how you can solve those challenges with a recommendation engine Familiarise yourself with machine learning algorithms provided by the Apache Spark framework Build different versions of recommendation engines from practical code examples Enhance the user experience by learning from user feedback Dive into the various techniques of recommender systems such as collaborative, content-based, and cross-recommendations In Detail With an increase of data in online e-commerce systems, the challenges in assisting users with narrowing down their search have grown dramatically. The various tools available in the Scala ecosystem enable developers to build a processing pipeline to meet those challenges and create a recommendation system to accelerate business growth and leverage brand advocacy for your clients. This book provides you with the Scala knowledge you need to build a recommendation engine. You'll be introduced to Scala and other related tools to set the stage for the project and familiarise yourself with the different stages in the data processing pipeline, including at which stages you can leverage the power of Scala and related tools. You'll also discover different machine learning algorithms using MLLib. As the book progresses, you will gain detailed knowledge of what constitutes a collaborative filtering based recommendation and explore different methods to improve users' recommendation. Style and approach A step-by-step guide full of real-world, hands-on examples of Scala recommendation engines. Each example is placed in context with explanation and visuals.","language":"en","currency":"USD","id":"1785282980","title":"Building a Recommendation Engine with Scala","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=xAQcDAAAQBAJ&source=gbs_api","authors":["Saleem Ansari"]},{"pageCount":207,"thumbnail":"http:\/\/books.google.com\/books\/content?id=8tslDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.72,"subtitle":"Building Real-World Big Data Systems on Azure HDInsight Using the Hadoop Ecosystem","description":"Get a jump start on using Azure HDInsight and Hadoop Ecosystem components. As most Hadoop and Big Data projects are written in either Java, Scala, or Python, this book minimizes the effort to learn another language and is written from the perspective of a .NET developer. Hadoop components are covered, including Hive, Pig, HBase, Storm, and Spark on Azure HDInsight, and code samples are written in .NET only. Processing Big Data with Azure HDInsight covers the fundamentals of big data, how businesses are using it to their advantage, and how Azure HDInsight fits into the big data world. This book introduces Hadoop and big data concepts and then dives into creating different solutions with HDInsight and the Hadoop Ecosystem. It covers concepts with real-world scenarios and code examples, making sure you get hands-on experience. The best way to utilize this book is to practice while reading. After reading this book you will be familiar with Azure HDInsight and how it can be utilized to build big data solutions, including batch processing, stream analytics, interactive processing, and storing and retrieving data in an efficient manner. What You'll Learn Understand the fundamentals of HDInsight and Hadoop Work with HDInsight cluster Query with Apache Hive and Apache Pig Store and retrieve data with Apache HBase Stream data processing using Apache Storm Work with Apache Spark Who This Book Is For Software developers, technical architects, data scientists\/analyts, and Hadoop administrators who want to develop on Microsoft\u2019s managed Hadoop offering, HDInsight","language":"en","currency":"USD","id":"1484228693","title":"Processing Big Data with Azure HDInsight","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=8tslDwAAQBAJ&source=gbs_api","authors":["Vinit Yadav"]},{"pageCount":192,"thumbnail":"http:\/\/books.google.com\/books\/content?id=o6jtBgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":14.59,"subtitle":null,"description":"If you are a professional or enthusiast who has a basic understanding of graphs or has basic knowledge of Neo4j operations, this is the book for you. Although it is targeted at an advanced user base, this book can be used by beginners as it touches upon the basics. So, if you are passionate about taming complex data with the help of graphs and building high performance applications, you will be able to get valuable insights from this book.","language":"en","currency":"USD","id":"1783555165","title":"Neo4j High Performance","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=o6jtBgAAQBAJ&source=gbs_api","authors":["Sonal Raj"]},{"pageCount":513,"thumbnail":"http:\/\/books.google.com\/books\/content?id=M_TKDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":0,"subtitle":null,"description":"This IBM¬Æ Redbooks¬Æ publication describes the features and functions the latest member of the IBM Z¬Æ platform, the IBM z15TM (machine type 8561). It includes information about the IBM z15 processor design, I\/O innovations, security features, and supported operating systems. The z15 is a state-of-the-art data and transaction system that delivers advanced capabilities, which are vital to any digital transformation. The z15 is designed for enhanced modularity, which is in an industry standard footprint. This system excels at the following tasks: Making use of multicloud integration services Securing data with pervasive encryption Accelerating digital transformation with agile service delivery Transforming a transactional platform into a data powerhouse Getting more out of the platform with IT Operational Analytics Accelerating digital transformation with agile service delivery Revolutionizing business processes Blending open source and Z technologies This book explains how this system uses new innovations and traditional Z strengths to satisfy growing demand for cloud, analytics, and open source technologies. With the z15 as the base, applications can run in a trusted, reliable, and secure environment that improves operations and lessens business risk.","language":"en","currency":"USD","id":"0738458120","title":"IBM z15 (8561) Technical Guide","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=M_TKDwAAQBAJ&source=gbs_api","authors":["Octavian Lascu","John Troy","Jannie Houlbjerg","Frank Packheiser","Paul Schouten","Kazuhiro Nakajima","Anna Shugol","Hervey Kamga","Bo XU","IBM Redbooks"]},{"pageCount":640,"thumbnail":"http:\/\/books.google.com\/books\/content?id=LauMDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":37.67,"subtitle":"with Big Data and Artificial Intelligence Case Studies","description":"The professional programmer\u2019s Deitel¬Æ guide to Python¬Æ with introductory artificial intelligence case studies Written for programmers with a background in another high-level language, Python for Programmers uses hands-on instruction to teach today\u2019s most compelling, leading-edge computing technologies and programming in Python\u2013one of the world\u2019s most popular and fastest-growing languages. Please read the Table of Contents diagram inside the front cover and the Preface for more details. In the context of 500+, real-world examples ranging from individual snippets to 40 large scripts and full implementation case studies, you\u2019ll use the interactive IPython interpreter with code in Jupyter Notebooks to quickly master the latest Python coding idioms. After covering Python Chapters 1-5 and a few key parts of Chapters 6-7, you\u2019ll be able to handle significant portions of the hands-on introductory AI case studies in Chapters 11-16, which are loaded with cool, powerful, contemporary examples. These include natural language processing, data mining Twitter¬Æ for sentiment analysis, cognitive computing with IBM¬Æ Watson‚Ñ¢, supervised machine learning with classification and regression, unsupervised machine learning with clustering, computer vision through deep learning and convolutional neural networks, deep learning with recurrent neural networks, big data with Hadoop¬Æ, Spark‚Ñ¢ and NoSQL databases, the Internet of Things and more. You\u2019ll also work directly or indirectly with cloud-based services, including Twitter, Google Translate‚Ñ¢, IBM Watson, Microsoft¬Æ Azure¬Æ, OpenMapQuest, PubNub and more. Features 500+ hands-on, real-world, live-code examples from snippets to case studies IPython + code in Jupyter¬Æ Notebooks Library-focused: Uses Python Standard Library and data science libraries to accomplish significant tasks with minimal code Rich Python coverage: Control statements, functions, strings, files, JSON serialization, CSV, exceptions Procedural, functional-style and object-oriented programming Collections: Lists, tuples, dictionaries, sets, NumPy arrays, pandas Series & DataFrames Static, dynamic and interactive visualizations Data experiences with real-world datasets and data sources Intro to Data Science sections: AI, basic stats, simulation, animation, random variables, data wrangling, regression AI, big data and cloud data science case studies: NLP, data mining Twitter¬Æ, IBM¬Æ Watson‚Ñ¢, machine learning, deep learning, computer vision, Hadoop¬Æ, Spark‚Ñ¢, NoSQL, IoT Open-source libraries: NumPy, pandas, Matplotlib, Seaborn, Folium, SciPy, NLTK, TextBlob, spaCy, Textatistic, Tweepy, scikit-learn¬Æ, Keras and more Accompanying code examples are available here: http:\/\/ptgmedia.pearsoncmg.com\/imprint_downloads\/informit\/bookreg\/9780135224335\/9780135224335_examples.zip. Register your product for convenient access to downloads, updates, and\/or corrections as they become available. See inside book for more information.","language":"en","currency":"USD","id":"0135231345","title":"Python for Programmers","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=LauMDwAAQBAJ&source=gbs_api","authors":["Paul J. Deitel","Harvey Deitel"]},{"pageCount":25,"thumbnail":"http:\/\/books.google.com\/books\/content?id=mAeuCAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":0,"subtitle":null,"description":"With the almost unfathomable increase in web traffic over recent years, driven by millions of connected users, businesses are gaining access to massive amounts of complex, unstructured data from which to gain insight. When Hadoop was introduced by Yahoo in 2007, it brought with it a paradigm shift in how this data was stored and analysed. Hadoop allowed small and medium sized companies to store huge amounts of data on cheap commodity servers in racks. The introduction of Big Data has allowed businesses to make decisions based on quantifiable analysis. Hadoop is now implemented in major organizations such as Amazon, IBM, Cloudera, and Dell to name a few. This book introduces you to Hadoop and to concepts such as \u2018MapReduce\u2019, \u2018Rack Awareness\u2019, \u2018Yarn\u2019 and \u2018HDFS Federation\u2019, which will help you get acquainted with the technology.","language":"en","currency":"USD","id":"1783552646","title":"Hadoop Explained","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=mAeuCAAAQBAJ&source=gbs_api","authors":["Aravind Shenoy"]},{"pageCount":418,"thumbnail":"http:\/\/books.google.com\/books\/content?id=4-ZDDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":null,"description":"Learn the basics of analytics on big data using Java, machine learning and other big data tools About This Book Acquire real-world set of tools for building enterprise level data science applications Surpasses the barrier of other languages in data science and learn create useful object-oriented codes Extensive use of Java compliant big data tools like apache spark, Hadoop, etc. Who This Book Is For This book is for Java developers who are looking to perform data analysis in production environment. Those who wish to implement data analysis in their Big data applications will find this book helpful. What You Will Learn Start from simple analytic tasks on big data Get into more complex tasks with predictive analytics on big data using machine learning Learn real time analytic tasks Understand the concepts with examples and case studies Prepare and refine data for analysis Create charts in order to understand the data See various real-world datasets In Detail This book covers case studies such as sentiment analysis on a tweet dataset, recommendations on a movielens dataset, customer segmentation on an ecommerce dataset, and graph analysis on actual flights dataset. This book is an end-to-end guide to implement analytics on big data with Java. Java is the de facto language for major big data environments, including Hadoop. This book will teach you how to perform analytics on big data with production-friendly Java. This book basically divided into two sections. The first part is an introduction that will help the readers get acquainted with big data environments, whereas the second part will contain a hardcore discussion on all the concepts in analytics on big data. It will take you from data analysis and data visualization to the core concepts and advantages of machine learning, real-life usage of regression and classification using Naive Bayes, a deep discussion on the concepts of clustering,and a review of simple neural networks on big data using deepLearning4j or plain Java Spark code. This book is a must-have book for Java developers who want to start learning big data analytics and want to use it in the real world. Style and approach The approach of book is to deliver practical learning modules in manageable content. Each chapter is a self-contained unit of a concept in big data analytics. Book will step by step builds the competency in the area of big data analytics. Examples using real world case studies to give ideas of real applications and how to use the techniques mentioned. The examples and case studies will be shown using both theory and code.","language":"en","currency":"USD","id":"1787282198","title":"Big Data Analytics with Java","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=4-ZDDwAAQBAJ&source=gbs_api","authors":["Rajat Mehta"]},{"pageCount":394,"thumbnail":"http:\/\/books.google.com\/books\/content?id=55lUDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":"Expert techniques for architecting end-to-end big data solutions to get valuable insights","description":"A comprehensive guide to design, build and execute effective Big Data strategies using Hadoop Key Features -Get an in-depth view of the Apache Hadoop ecosystem and an overview of the architectural patterns pertaining to the popular Big Data platform -Conquer different data processing and analytics challenges using a multitude of tools such as Apache Spark, Elasticsearch, Tableau and more -A comprehensive, step-by-step guide that will teach you everything you need to know, to be an expert Hadoop Architect Book Description The complex structure of data these days requires sophisticated solutions for data transformation, to make the information more accessible to the users.This book empowers you to build such solutions with relative ease with the help of Apache Hadoop, along with a host of other Big Data tools. This book will give you a complete understanding of the data lifecycle management with Hadoop, followed by modeling of structured and unstructured data in Hadoop. It will also show you how to design real-time streaming pipelines by leveraging tools such as Apache Spark, and build efficient enterprise search solutions using Elasticsearch. You will learn to build enterprise-grade analytics solutions on Hadoop, and how to visualize your data using tools such as Apache Superset. This book also covers techniques for deploying your Big Data solutions on the cloud Apache Ambari, as well as expert techniques for managing and administering your Hadoop cluster. By the end of this book, you will have all the knowledge you need to build expert Big Data systems. What you will learn Build an efficient enterprise Big Data strategy centered around Apache Hadoop Gain a thorough understanding of using Hadoop with various Big Data frameworks such as Apache Spark, Elasticsearch and more Set up and deploy your Big Data environment on premises or on the cloud with Apache Ambari Design effective streaming data pipelines and build your own enterprise search solutions Utilize the historical data to build your analytics solutions and visualize them using popular tools such as Apache Superset Plan, set up and administer your Hadoop cluster efficiently Who this book is for This book is for Big Data professionals who want to fast-track their career in the Hadoop industry and become an expert Big Data architect. Project managers and mainframe professionals looking forward to build a career in Big Data Hadoop will also find this book to be useful. Some understanding of Hadoop is required to get the best out of this book.","language":"en","currency":"USD","id":"1787128814","title":"Modern Big Data Processing with Hadoop","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=55lUDwAAQBAJ&source=gbs_api","authors":["V Naresh Kumar","Prashant Shindgikar"]},{"pageCount":256,"thumbnail":"http:\/\/books.google.com\/books\/content?id=RHeuDQAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":"Designing and Building Effective Analytics at Scale","description":"The Complete Guide to Data Science with Hadoop\u2014For Technical Professionals, Businesspeople, and Students Demand is soaring for professionals who can solve real data science problems with Hadoop and Spark. Practical Data Science with Hadoop¬Æ and Spark is your complete guide to doing just that. Drawing on immense experience with Hadoop and big data, three leading experts bring together everything you need: high-level concepts, deep-dive techniques, real-world use cases, practical applications, and hands-on tutorials. The authors introduce the essentials of data science and the modern Hadoop ecosystem, explaining how Hadoop and Spark have evolved into an effective platform for solving data science problems at scale. In addition to comprehensive application coverage, the authors also provide useful guidance on the important steps of data ingestion, data munging, and visualization. Once the groundwork is in place, the authors focus on specific applications, including machine learning, predictive modeling for sentiment analysis, clustering for document analysis, anomaly detection, and natural language processing (NLP). This guide provides a strong technical foundation for those who want to do practical data science, and also presents business-driven guidance on how to apply Hadoop and Spark to optimize ROI of data science initiatives. Learn What data science is, how it has evolved, and how to plan a data science career How data volume, variety, and velocity shape data science use cases Hadoop and its ecosystem, including HDFS, MapReduce, YARN, and Spark Data importation with Hive and Spark Data quality, preprocessing, preparation, and modeling Visualization: surfacing insights from huge data sets Machine learning: classification, regression, clustering, and anomaly detection Algorithms and Hadoop tools for predictive modeling Cluster analysis and similarity functions Large-scale anomaly detection NLP: applying data science to human language","language":"en","currency":"USD","id":"0134029720","title":"Practical Data Science with Hadoop and Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=RHeuDQAAQBAJ&source=gbs_api","authors":["Ofer Mendelevitch","Casey Stella","Douglas Eadline"]},{"pageCount":482,"thumbnail":"http:\/\/books.google.com\/books\/content?id=MHxeDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":19.79,"subtitle":"Build highly effective analytics solutions to gain valuable insight into your big data","description":"Explore big data concepts, platforms, analytics, and their applications using the power of Hadoop 3 Key Features Learn Hadoop 3 to build effective big data analytics solutions on-premise and on cloud Integrate Hadoop with other big data tools such as R, Python, Apache Spark, and Apache Flink Exploit big data using Hadoop 3 with real-world examples Book Description Apache Hadoop is the most popular platform for big data processing, and can be combined with a host of other big data tools to build powerful analytics solutions. Big Data Analytics with Hadoop 3 shows you how to do just that, by providing insights into the software as well as its benefits with the help of practical examples. Once you have taken a tour of Hadoop 3\u2019s latest features, you will get an overview of HDFS, MapReduce, and YARN, and how they enable faster, more efficient big data processing. You will then move on to learning how to integrate Hadoop with the open source tools, such as Python and R, to analyze and visualize data and perform statistical computing on big data. As you get acquainted with all this, you will explore how to use Hadoop 3 with Apache Spark and Apache Flink for real-time data analytics and stream processing. In addition to this, you will understand how to use Hadoop to build analytics solutions on the cloud and an end-to-end pipeline to perform big data analysis using practical use cases. By the end of this book, you will be well-versed with the analytical capabilities of the Hadoop ecosystem. You will be able to build powerful solutions to perform big data analytics and get insight effortlessly. What you will learn Explore the new features of Hadoop 3 along with HDFS, YARN, and MapReduce Get well-versed with the analytical capabilities of Hadoop ecosystem using practical examples Integrate Hadoop with R and Python for more efficient big data processing Learn to use Hadoop with Apache Spark and Apache Flink for real-time data analytics Set up a Hadoop cluster on AWS cloud Perform big data analytics on AWS using Elastic Map Reduce Who this book is for Big Data Analytics with Hadoop 3 is for you if you are looking to build high-performance analytics solutions for your enterprise or business using Hadoop 3\u2019s powerful features, or you\u2019re new to big data analytics. A basic understanding of the Java programming language is required.","language":"en","currency":"USD","id":"1788624955","title":"Big Data Analytics with Hadoop 3","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=MHxeDwAAQBAJ&source=gbs_api","authors":["Sridhar Alla"]},{"pageCount":304,"thumbnail":"http:\/\/books.google.com\/books\/content?id=wWCYBgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":63.99,"subtitle":"Case Studies with Hadoop, Scalding and Spark","description":"This timely text\/reference describes the development and implementation of large-scale distributed processing systems using open source tools and technologies. Comprehensive in scope, the book presents state-of-the-art material on building high performance distributed computing systems, providing practical guidance and best practices as well as describing theoretical software frameworks. Features: describes the fundamentals of building scalable software systems for large-scale data processing in the new paradigm of high performance distributed computing; presents an overview of the Hadoop ecosystem, followed by step-by-step instruction on its installation, programming and execution; Reviews the basics of Spark, including resilient distributed datasets, and examines Hadoop streaming and working with Scalding; Provides detailed case studies on approaches to clustering, data classification and regression analysis; Explains the process of creating a working recommender system using Scalding and Spark.","language":"en","currency":"USD","id":"3319134973","title":"Guide to High Performance Distributed Computing","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=wWCYBgAAQBAJ&source=gbs_api","authors":["K.G. Srinivasa","Anil Kumar Muppalla"]},{"pageCount":256,"thumbnail":"http:\/\/books.google.com\/books\/content?id=S69PCwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.72,"subtitle":null,"description":"In this fast-paced book on the Docker open standards platform for developing, packaging and running portable distributed applications, Deepak Vorhadiscusses how to build, ship and run applications on any platform such as a PC, the cloud, data center or a virtual machine. He describes how to install and create Docker images. and the advantages off Docker containers.The remainder of the book is devoted to discussing using Docker with important software solutions. He begins by discussing using Docker with a traditional RDBMS using Oracle and MySQL. Next he moves on to NoSQL with chapter on MongoDB Cassandra, and Couchbase. Then he addresses the use of Docker in the Hadoop ecosystem with complete chapters on utilizing not only Hadoop, but Hive, HBase, Sqoop, Kafka, Solr and Spark. What You Will Learn How to install a Docker image How to create a Docker container How to run an Application in a Docker Container Use Docker with Apache Hadoop Ecosystem Use Docker with NoSQL Databases Use Docker with RDBMS Who This Book Is ForApache Hadoop Developers. Database developers. NoSQL Developers.","language":"en","currency":"USD","id":"1484218302","title":"Pro Docker","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=S69PCwAAQBAJ&source=gbs_api","authors":["Deepak Vohra"]},{"pageCount":280,"thumbnail":"http:\/\/books.google.com\/books\/content?id=uJonDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":40.79,"subtitle":"Patterns for Learning from Data at Scale","description":"In the second edition of this practical book, four Cloudera data scientists present a set of self-contained patterns for performing large-scale data analysis with Spark. The authors bring Spark, statistical methods, and real-world data sets together to teach you how to approach analytics problems by example. Updated for Spark 2.1, this edition acts as an introduction to these techniques and other best practices in Spark programming. You\u2019ll start with an introduction to Spark and its ecosystem, and then dive into patterns that apply common techniques\u2014including classification, clustering, collaborative filtering, and anomaly detection\u2014to fields such as genomics, security, and finance. If you have an entry-level understanding of machine learning and statistics, and you program in Java, Python, or Scala, you\u2019ll find the book\u2019s patterns useful for working on your own data applications. With this book, you will: Familiarize yourself with the Spark programming model Become comfortable within the Spark ecosystem Learn general approaches in data science Examine complete implementations that analyze large public data sets Discover which machine learning tools make sense for particular problems Acquire code that can be adapted to many uses","language":"en","currency":"USD","id":"1491972904","title":"Advanced Analytics with Spark","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=uJonDwAAQBAJ&source=gbs_api","authors":["Sandy Ryza","Uri Laserson","Sean Owen","Josh Wills"]},{"pageCount":506,"thumbnail":"http:\/\/books.google.com\/books\/content?id=s-nUDQAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.72,"subtitle":null,"description":"Utilize R to uncover hidden patterns in your Big Data About This Book Perform computational analyses on Big Data to generate meaningful results Get a practical knowledge of R programming language while working on Big Data platforms like Hadoop, Spark, H2O and SQL\/NoSQL databases, Explore fast, streaming, and scalable data analysis with the most cutting-edge technologies in the market Who This Book Is For This book is intended for Data Analysts, Scientists, Data Engineers, Statisticians, Researchers, who want to integrate R with their current or future Big Data workflows. It is assumed that readers have some experience in data analysis and understanding of data management and algorithmic processing of large quantities of data, however they may lack specific skills related to R. What You Will Learn Learn about current state of Big Data processing using R programming language and its powerful statistical capabilities Deploy Big Data analytics platforms with selected Big Data tools supported by R in a cost-effective and time-saving manner Apply the R language to real-world Big Data problems on a multi-node Hadoop cluster, e.g. electricity consumption across various socio-demographic indicators and bike share scheme usage Explore the compatibility of R with Hadoop, Spark, SQL and NoSQL databases, and H2O platform In Detail Big Data analytics is the process of examining large and complex data sets that often exceed the computational capabilities. R is a leading programming language of data science, consisting of powerful functions to tackle all problems related to Big Data processing. The book will begin with a brief introduction to the Big Data world and its current industry standards. With introduction to the R language and presenting its development, structure, applications in real world, and its shortcomings. Book will progress towards revision of major R functions for data management and transformations. Readers will be introduce to Cloud based Big Data solutions (e.g. Amazon EC2 instances and Amazon RDS, Microsoft Azure and its HDInsight clusters) and also provide guidance on R connectivity with relational and non-relational databases such as MongoDB and HBase etc. It will further expand to include Big Data tools such as Apache Hadoop ecosystem, HDFS and MapReduce frameworks. Also other R compatible tools such as Apache Spark, its machine learning library Spark MLlib, as well as H2O. Style and approach This book will serve as a practical guide to tackling Big Data problems using R programming language and its statistical environment. Each section of the book will present you with concise and easy-to-follow steps on how to process, transform and analyse large data sets.","language":"en","currency":"USD","id":"1786463725","title":"Big Data Analytics with R","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=s-nUDQAAQBAJ&source=gbs_api","authors":["Simon Walkowiak"]},{"pageCount":260,"thumbnail":"http:\/\/books.google.com\/books\/content?id=eXPLDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":40.79,"subtitle":"Going from Idea to Product","description":"Learn the skills necessary to design, build, and deploy applications powered by machine learning (ML). Through the course of this hands-on book, you\u2019ll build an example ML-driven application from initial idea to deployed product. Data scientists, software engineers, and product managers\u2014including experienced practitioners and novices alike\u2014will learn the tools, best practices, and challenges involved in building a real-world ML application step by step. Author Emmanuel Ameisen, an experienced data scientist who led an AI education program, demonstrates practical ML concepts using code snippets, illustrations, screenshots, and interviews with industry leaders. Part I teaches you how to plan an ML application and measure success. Part II explains how to build a working ML model. Part III demonstrates ways to improve the model until it fulfills your original vision. Part IV covers deployment and monitoring strategies. This book will help you: Define your product goal and set up a machine learning problem Build your first end-to-end pipeline quickly and acquire an initial dataset Train and evaluate your ML models and address performance bottlenecks Deploy and monitor your models in a production environment","language":"en","currency":"USD","id":"1492045063","title":"Building Machine Learning Powered Applications","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=eXPLDwAAQBAJ&source=gbs_api","authors":["Emmanuel Ameisen"]},{"pageCount":560,"thumbnail":"http:\/\/books.google.com\/books\/content?id=xRcaEAAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":48,"subtitle":null,"description":"Everything you need to succeed on the Google Cloud Certified Professional Cloud Architect exam in one accessible study guide Take the challenging Google Cloud Certified Professional Cloud Architect exam with confidence using the comprehensive information contained in this invaluable self-study guide. The book provides a thorough overview of cloud architecture and Google Cloud Platform (GCP) and shows you how to pass the test. Beyond exam preparation, the guide also serves as a valuable on-the-job reference. Written by a recognized expert in the field, Google Cloud Certified Professional Cloud Architect All-In-One Exam Guideis based on proven pedagogy and features special elements that teach and reinforce practical skills. The book contains accurate practice questions and in-depth explanations. You will discover how to design, develop, and manage robust, secure, scalable, and highly available solutions to drive business objectives. Offers 100% coverage of every objective for the Google Cloud Certified Professional Cloud Architect exam Online content includes 100 additional practice questions in the TotalTester customizable exam engine Written by a Google Cloud Certified Professional Cloud Architect","language":"en","currency":"USD","id":"1264257287","title":"Google Cloud Certified Professional Cloud Architect All-in-One Exam Guide","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=xRcaEAAAQBAJ&source=gbs_api","authors":["Iman Ghanizada"]},{"pageCount":979,"thumbnail":"http:\/\/books.google.com\/books\/content?id=LWzWDQAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":55.99,"subtitle":null,"description":"Unlock the power of your data with Hadoop 2.X ecosystem and its data warehousing techniques across large data sets About This Book Conquer the mountain of data using Hadoop 2.X tools The authors succeed in creating a context for Hadoop and its ecosystem Hands-on examples and recipes giving the bigger picture and helping you to master Hadoop 2.X data processing platforms Overcome the challenging data processing problems using this exhaustive course with Hadoop 2.X Who This Book Is For This course is for Java developers, who know scripting, wanting a career shift to Hadoop - Big Data segment of the IT industry. So if you are a novice in Hadoop or an expert, this book will make you reach the most advanced level in Hadoop 2.X. What You Will Learn Best practices for setup and configuration of Hadoop clusters, tailoring the system to the problem at hand Integration with relational databases, using Hive for SQL queries and Sqoop for data transfer Installing and maintaining Hadoop 2.X cluster and its ecosystem Advanced Data Analysis using the Hive, Pig, and Map Reduce programs Machine learning principles with libraries such as Mahout and Batch and Stream data processing using Apache Spark Understand the changes involved in the process in the move from Hadoop 1.0 to Hadoop 2.0 Dive into YARN and Storm and use YARN to integrate Storm with Hadoop Deploy Hadoop on Amazon Elastic MapReduce and Discover HDFS replacements and learn about HDFS Federation In Detail As Marc Andreessen has said \u201CData is eating the world,\u201D which can be witnessed today being the age of Big Data, businesses are producing data in huge volumes every day and this rise in tide of data need to be organized and analyzed in a more secured way. With proper and effective use of Hadoop, you can build new-improved models, and based on that you will be able to make the right decisions. The first module, Hadoop beginners Guide will walk you through on understanding Hadoop with very detailed instructions and how to go about using it. Commands are explained using sections called \u201CWhat just happened\u201D for more clarity and understanding. The second module, Hadoop Real World Solutions Cookbook, 2nd edition, is an essential tutorial to effectively implement a big data warehouse in your business, where you get detailed practices on the latest technologies such as YARN and Spark. Big data has become a key basis of competition and the new waves of productivity growth. Hence, once you get familiar with the basics and implement the end-to-end big data use cases, you will start exploring the third module, Mastering Hadoop. So, now the question is if you need to broaden your Hadoop skill set to the next level after you nail the basics and the advance concepts, then this course is indispensable. When you finish this course, you will be able to tackle the real-world scenarios and become a big data expert using the tools and the knowledge based on the various step-by-step tutorials and recipes. Style and approach This course has covered everything right from the basic concepts of Hadoop till you master the advance mechanisms to become a big data expert. The goal here is to help you learn the basic essentials using the step-by-step tutorials and from there moving toward the recipes with various real-world solutions for you. It covers all the important aspects of Hadoop from system designing and configuring Hadoop, machine learning principles with various libraries with chapters illustrated with code fragments and schematic diagrams. This is a compendious course to explore Hadoop from the basics to the most advanced techniques available in Hadoop 2.X.","language":"en","currency":"USD","id":"1787120457","title":"Hadoop: Data Processing and Modelling","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=LWzWDQAAQBAJ&source=gbs_api","authors":["Garry Turkington","Tanmay Deshpande","Sandeep Karanth"]},{"pageCount":467,"thumbnail":"http:\/\/books.google.com\/books\/content?id=-WZCDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.19,"subtitle":null,"description":"Follow this handbook to build, configure, tune, and secure Apache Cassandra databases. Start with the installation of Cassandra and move on to the creation of a single instance, and then a cluster of Cassandra databases. Cassandra is increasingly a key player in many big data environments, and this book shows you how to use Cassandra with Apache Spark, a popular big data processing framework. Also covered are day-to-day topics of importance such as the backup and recovery of Cassandra databases, using the right compression and compaction strategies, and loading and unloading data. Expert Apache Cassandra Administration provides numerous step-by-step examples starting with the basics of a Cassandra database, and going all the way through backup and recovery, performance optimization, and monitoring and securing the data. The book serves as an authoritative and comprehensive guide to the building and management of simple to complex Cassandra databases. The book: Takes you through building a Cassandra database from installation of the software and creation of a single database, through to complex clusters and data centers Provides numerous examples of actual commands in a real-life Cassandra environment that show how to confidently configure, manage, troubleshoot, and tune Cassandra databases Shows how to use the Cassandra configuration properties to build a highly stable, available, and secure Cassandra database that always operates at peak efficiency What You'll Learn Install the Cassandra software and create your first database Understand the Cassandra data model, and the internal architecture of a Cassandra database Create your own Cassandra cluster, step-by-step Run a Cassandra cluster on Docker Work with Apache Spark by connecting to a Cassandra database Deploy Cassandra clusters in your data center, or on Amazon EC2 instances Back up and restore mission-critical Cassandra databases Monitor, troubleshoot, and tune production Cassandra databases, and cut your spending on resources such as memory, servers, and storage Who This Book Is For Database administrators, developers, and architects who are looking for an authoritative and comprehensive single volume for all their Cassandra administration needs. Also for administrators who are tasked with setting up and maintaining highly reliable and high-performing Cassandra databases. An excellent choice for big data administrators, database administrators, architects, and developers who use Cassandra as their key data store, to support high volume online transactions, or as a decentralized, elastic data store.","language":"en","currency":"USD","id":"1484231260","title":"Expert Apache Cassandra Administration","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=-WZCDwAAQBAJ&source=gbs_api","authors":["Sam R. Alapati"]},{"pageCount":250,"thumbnail":"http:\/\/books.google.com\/books\/content?id=yEzSCgAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":15.39,"subtitle":"Learn the Essentials of Big Data Computing in the Apache Hadoop 2 Ecosystem","description":"Get Started Fast with Apache Hadoop¬Æ 2, YARN, and Today\u2019s Hadoop Ecosystem With Hadoop 2.x and YARN, Hadoop moves beyond MapReduce to become practical for virtually any type of data processing. Hadoop 2.x and the Data Lake concept represent a radical shift away from conventional approaches to data usage and storage. Hadoop 2.x installations offer unmatched scalability and breakthrough extensibility that supports new and existing Big Data analytics processing methods and models. Hadoop¬Æ 2 Quick-Start Guide is the first easy, accessible guide to Apache Hadoop 2.x, YARN, and the modern Hadoop ecosystem. Building on his unsurpassed experience teaching Hadoop and Big Data, author Douglas Eadline covers all the basics you need to know to install and use Hadoop 2 on personal computers or servers, and to navigate the powerful technologies that complement it. Eadline concisely introduces and explains every key Hadoop 2 concept, tool, and service, illustrating each with a simple \u201Cbeginning-to-end\u201D example and identifying trustworthy, up-to-date resources for learning more. This guide is ideal if you want to learn about Hadoop 2 without getting mired in technical details. Douglas Eadline will bring you up to speed quickly, whether you\u2019re a user, admin, devops specialist, programmer, architect, analyst, or data scientist. Coverage Includes Understanding what Hadoop 2 and YARN do, and how they improve on Hadoop 1 with MapReduce Understanding Hadoop-based Data Lakes versus RDBMS Data Warehouses Installing Hadoop 2 and core services on Linux machines, virtualized sandboxes, or clusters Exploring the Hadoop Distributed File System (HDFS) Understanding the essentials of MapReduce and YARN application programming Simplifying programming and data movement with Apache Pig, Hive, Sqoop, Flume, Oozie, and HBase Observing application progress, controlling jobs, and managing workflows Managing Hadoop efficiently with Apache Ambari\u2013including recipes for HDFS to NFSv3 gateway, HDFS snapshots, and YARN configuration Learning basic Hadoop 2 troubleshooting, and installing Apache Hue and Apache Spark","language":"en","currency":"USD","id":"0134049993","title":"Hadoop 2 Quick-Start Guide","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=yEzSCgAAQBAJ&source=gbs_api","authors":["Douglas Eadline"]},{"pageCount":696,"thumbnail":"http:\/\/books.google.com\/books\/content?id=JVQoDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":31.72,"subtitle":null,"description":"Over 170 advanced recipes to search, analyze, deploy, manage, and monitor data effectively with Elasticsearch 5.x About This Book Deploy and manage simple Elasticsearch nodes as well as complex cluster topologies Write native plugins to extend the functionalities of Elasticsearch 5.x to boost your business Packed with clear, step-by-step recipes to walk you through the capabilities of Elasticsearch 5.x Who This Book Is For If you are a developer who wants to get the most out of Elasticsearch for advanced search and analytics, this is the book for you. Some understanding of JSON is expected. If you want to extend Elasticsearch, understanding of Java and related technologies is also required. What You Will Learn Choose the best Elasticsearch cloud topology to deploy and power it up with external plugins Develop tailored mapping to take full control of index steps Build complex queries through managing indices and documents Optimize search results through executing analytics aggregations Monitor the performance of the cluster and nodes Install Kibana to monitor cluster and extend Kibana for plugins Integrate Elasticsearch in Java, Scala, Python and Big Data applications In Detail Elasticsearch is a Lucene-based distributed search server that allows users to index and search unstructured content with petabytes of data. This book is your one-stop guide to master the complete Elasticsearch ecosystem. We'll guide you through comprehensive recipes on what's new in Elasticsearch 5.x, showing you how to create complex queries and analytics, and perform index mapping, aggregation, and scripting. Further on, you will explore the modules of Cluster and Node monitoring and see ways to back up and restore a snapshot of an index. You will understand how to install Kibana to monitor a cluster and also to extend Kibana for plugins. Finally, you will also see how you can integrate your Java, Scala, Python, and Big Data applications such as Apache Spark and Pig with Elasticsearch, and add enhanced functionalities with custom plugins. By the end of this book, you will have an in-depth knowledge of the implementation of the Elasticsearch architecture and will be able to manage data efficiently and effectively with Elasticsearch. Style and approach This book follows a problem-solution approach to effectively use and manage Elasticsearch. Each recipe focuses on a particular task at hand, and is explained in a very simple, easy to understand manner.","language":"en","currency":"USD","id":"1786466880","title":"Elasticsearch 5.x Cookbook","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=JVQoDwAAQBAJ&source=gbs_api","authors":["Alberto Paro"]},{"pageCount":250,"thumbnail":"http:\/\/books.google.com\/books\/content?id=SPNFDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":17.19,"subtitle":"Over 100 practical recipes on using distributed enterprise messaging to handle real-time data","description":"Simplify real-time data processing by leveraging the power of Apache Kafka 1.0 Key Features Use Kafka 1.0 features such as Confluent platforms and Kafka streams to build efficient streaming data applications to handle and process your data Integrate Kafka with other Big Data tools such as Apache Hadoop, Apache Spark, and more Hands-on recipes to help you design, operate, maintain, and secure your Apache Kafka cluster with ease Book Description Apache Kafka provides a unified, high-throughput, low-latency platform to handle real-time data feeds. This book will show you how to use Kafka efficiently, and contains practical solutions to the common problems that developers and administrators usually face while working with it. This practical guide contains easy-to-follow recipes to help you set up, configure, and use Apache Kafka in the best possible manner. You will use Apache Kafka Consumers and Producers to build effective real-time streaming applications. The book covers the recently released Kafka version 1.0, the Confluent Platform and Kafka Streams. The programming aspect covered in the book will teach you how to perform important tasks such as message validation, enrichment and composition.Recipes focusing on optimizing the performance of your Kafka cluster, and integrate Kafka with a variety of third-party tools such as Apache Hadoop, Apache Spark, and Elasticsearch will help ease your day to day collaboration with Kafka greatly. Finally, we cover tasks related to monitoring and securing your Apache Kafka cluster using tools such as Ganglia and Graphite. If you're looking to become the go-to person in your organization when it comes to working with Apache Kafka, this book is the only resource you need to have. What you will learn -Install and configure Apache Kafka 1.0 to get optimal performance -Create and configure Kafka Producers and Consumers -Operate your Kafka clusters efficiently by implementing the mirroring technique -Work with the new Confluent platform and Kafka streams, and achieve high availability with Kafka -Monitor Kafka using tools such as Graphite and Ganglia -Integrate Kafka with third-party tools such as Elasticsearch, Logstash, Apache Hadoop, Apache Spark, and more Who this book is for This book is for developers and Kafka administrators who are looking for quick, practical solutions to problems encountered while operating, managing or monitoring Apache Kafka. If you are a developer, some knowledge of Scala or Java will help, while for administrators, some working knowledge of Kafka will be useful.","language":"en","currency":"USD","id":"178728218X","title":"Apache Kafka 1.0 Cookbook","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=SPNFDwAAQBAJ&source=gbs_api","authors":["Ra√∫l Estrada"]},{"pageCount":596,"thumbnail":"http:\/\/books.google.com\/books\/content?id=nHc5DwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":11.34,"subtitle":null,"description":"A practical guide to implementing your enterprise data lake using Lambda Architecture as the base About This Book Build a full-fledged data lake for your organization with popular big data technologies using the Lambda architecture as the base Delve into the big data technologies required to meet modern day business strategies A highly practical guide to implementing enterprise data lakes with lots of examples and real-world use-cases Who This Book Is For Java developers and architects who would like to implement a data lake for their enterprise will find this book useful. If you want to get hands-on experience with the Lambda Architecture and big data technologies by implementing a practical solution using these technologies, this book will also help you. What You Will Learn Build an enterprise-level data lake using the relevant big data technologies Understand the core of the Lambda architecture and how to apply it in an enterprise Learn the technical details around Sqoop and its functionalities Integrate Kafka with Hadoop components to acquire enterprise data Use flume with streaming technologies for stream-based processing Understand stream- based processing with reference to Apache Spark Streaming Incorporate Hadoop components and know the advantages they provide for enterprise data lakes Build fast, streaming, and high-performance applications using ElasticSearch Make your data ingestion process consistent across various data formats with configurability Process your data to derive intelligence using machine learning algorithms In Detail The term \"Data Lake\" has recently emerged as a prominent term in the big data industry. Data scientists can make use of it in deriving meaningful insights that can be used by businesses to redefine or transform the way they operate. Lambda architecture is also emerging as one of the very eminent patterns in the big data landscape, as it not only helps to derive useful information from historical data but also correlates real-time data to enable business to take critical decisions. This book tries to bring these two important aspects \u2014 data lake and lambda architecture\u2014together. This book is divided into three main sections. The first introduces you to the concept of data lakes, the importance of data lakes in enterprises, and getting you up-to-speed with the Lambda architecture. The second section delves into the principal components of building a data lake using the Lambda architecture. It introduces you to popular big data technologies such as Apache Hadoop, Spark, Sqoop, Flume, and ElasticSearch. The third section is a highly practical demonstration of putting it all together, and shows you how an enterprise data lake can be implemented, along with several real-world use-cases. It also shows you how other peripheral components can be added to the lake to make it more efficient. By the end of this book, you will be able to choose the right big data technologies using the lambda architectural patterns to build your enterprise data lake. Style and approach The book takes a pragmatic approach, showing ways to leverage big data technologies and lambda architecture to build an enterprise-level data lake.","language":"en","currency":"USD","id":"1787282651","title":"Data Lake for Enterprises","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=nHc5DwAAQBAJ&source=gbs_api","authors":["Tomcy John","Pankaj Misra"]},{"pageCount":1100,"thumbnail":"http:\/\/books.google.com\/books\/content?id=QVQoDwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":57.59,"subtitle":null,"description":"Scala will be a valuable tool to have on hand during your data science journey for everything from data cleaning to cutting-edge machine learning About This Book Build data science and data engineering solutions with ease An in-depth look at each stage of the data analysis process \u2014 from reading and collecting data to distributed analytics Explore a broad variety of data processing, machine learning, and genetic algorithms through diagrams, mathematical formulations, and source code Who This Book Is For This learning path is perfect for those who are comfortable with Scala programming and now want to enter the field of data science. Some knowledge of statistics is expected. What You Will Learn Transfer and filter tabular data to extract features for machine learning Read, clean, transform, and write data to both SQL and NoSQL databases Create Scala web applications that couple with JavaScript libraries such as D3 to create compelling interactive visualizations Load data from HDFS and HIVE with ease Run streaming and graph analytics in Spark for exploratory analysis Bundle and scale up Spark jobs by deploying them into a variety of cluster managers Build dynamic workflows for scientific computing Leverage open source libraries to extract patterns from time series Master probabilistic models for sequential data In Detail Scala is especially good for analyzing large sets of data as the scale of the task doesn't have any significant impact on performance. Scala's powerful functional libraries can interact with databases and build scalable frameworks \u2014 resulting in the creation of robust data pipelines. The first module introduces you to Scala libraries to ingest, store, manipulate, process, and visualize data. Using real world examples, you will learn how to design scalable architecture to process and model data \u2014 starting from simple concurrency constructs and progressing to actor systems and Apache Spark. After this, you will also learn how to build interactive visualizations with web frameworks. Once you have become familiar with all the tasks involved in data science, you will explore data analytics with Scala in the second module. You'll see how Scala can be used to make sense of data through easy to follow recipes. You will learn about Bokeh bindings for exploratory data analysis and quintessential machine learning with algorithms with Spark ML library. You'll get a sufficient understanding of Spark streaming, machine learning for streaming data, and Spark graphX. Armed with a firm understanding of data analysis, you will be ready to explore the most cutting-edge aspect of data science \u2014 machine learning. The final module teaches you the A to Z of machine learning with Scala. You'll explore Scala for dependency injections and implicits, which are used to write machine learning algorithms. You'll also explore machine learning topics such as clustering, dimentionality reduction, Naive Bayes, Regression models, SVMs, neural networks, and more. This learning path combines some of the best that Packt has to offer into one complete, curated package. It includes content from the following Packt products: Scala for Data Science, Pascal Bugnion Scala Data Analysis Cookbook, Arun Manivannan Scala for Machine Learning, Patrick R. Nicolas Style and approach A complete package with all the information necessary to start building useful data engineering and data science solutions straight away. It contains a diverse set of recipes that cover the full spectrum of interesting data analysis tasks and will help you revolutionize your data analysis skills using Scala.","language":"en","currency":"USD","id":"1787281035","title":"Scala: Guide for Data Science Professionals","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=QVQoDwAAQBAJ&source=gbs_api","authors":["Pascal Bugnion","Arun Manivannan","Patrick R. Nicolas"]},{"pageCount":73,"thumbnail":"http:\/\/books.google.com\/books\/content?id=uaSSAwAAQBAJ&printsec=frontcover&img=1&zoom=1&edge=curl&source=gbs_api","price":25,"subtitle":null,"description":"Many applications process high volumes of streaming data, among them Internet traffic analysis, financial tickers, and transaction log mining. In general, a data stream is an unbounded data set that is produced incrementally over time, rather than being available in full before its processing begins. In this lecture, we give an overview of recent research in stream processing, ranging from answering simple queries on high-speed streams to loading real-time data feeds into a streaming warehouse for off-line analysis. We will discuss two types of systems for end-to-end stream processing: Data Stream Management Systems (DSMSs) and Streaming Data Warehouses (SDWs). A traditional database management system typically processes a stream of ad-hoc queries over relatively static data. In contrast, a DSMS evaluates static (long-running) queries on streaming data, making a single pass over the data and using limited working memory. In the first part of this lecture, we will discuss research problems in DSMSs, such as continuous query languages, non-blocking query operators that continually react to new data, and continuous query optimization. The second part covers SDWs, which combine the real-time response of a DSMS by loading new data as soon as they arrive with a data warehouse's ability to manage Terabytes of historical data on secondary storage. Table of Contents: Introduction \/ Data Stream Management Systems \/ Streaming Data Warehouses \/ Conclusions","language":"en","currency":"USD","id":"1608452735","title":"Data Stream Management","infoLink":"https:\/\/play.google.com\/store\/books\/details?id=uaSSAwAAQBAJ&source=gbs_api","authors":["Lukasz Golab","M. Tamer Ozsu"]}]